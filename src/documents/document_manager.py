"""
High level orchestration layer for document-related processors.
"""

from __future__ import annotations

import asyncio
import asyncio
import html
import json
import logging
import re
import tempfile
import uuid
from datetime import datetime
from dataclasses import asdict
from html import escape as html_escape
from pathlib import Path
from types import MappingProxyType
from typing import Any, Awaitable, Callable, Dict, List, Mapping

from src.core.settings import AppSettings

from .anonymizer import DocumentAnonymizer
from .base import DocumentInfo, DocumentResult, DocumentStorage, ProcessingError
from .document_chat import DocumentChat
from .document_drafter import DocumentDraftingError, build_docx_from_markdown
from .lawsuit_analyzer import LawsuitAnalyzer
from .ocr_converter import OCRConverter
from .risk_analyzer import RiskAnalyzer
from .storage_backends import ArtifactUploader, S3ArtifactUploader
from .summarizer import DocumentSummarizer
from .translator import DocumentTranslator
from .utils import TextProcessor, write_text_async

_LAWSUIT_SECTION_META: dict[str, tuple[str, str]] = {
    "demands": ("üìù", "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è"),
    "legal_basis": ("üìö", "–ü—Ä–∞–≤–æ–≤–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ"),
    "evidence": ("üìÅ", "–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞"),
    "strengths": ("‚úÖ", "–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã"),
    "risks": ("‚ö†Ô∏è", "–†–∏—Å–∫–∏ –∏ —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞"),
    "missing_elements": ("‚ùó", "–ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã"),
    "recommendations": ("üí°", "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"),
    "procedural_notes": ("üìÖ", "–ü—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏"),
}

logger = logging.getLogger(__name__)

_FILENAME_SANITIZE_RE = re.compile(r"[\\/:*?\"<>|\r\n]+")
_FILENAME_WHITESPACE_RE = re.compile(r"\s+")


class DocumentManager:
    """Facade that wires document processors, storage and formatting."""

    def __init__(self, openai_service=None, settings: AppSettings | None = None):
        if settings is None:
            from src.core.app_context import get_settings  # avoid circular import

            settings = get_settings()
        self._settings = settings
        self._openai_service = openai_service

        self.storage = self._init_storage(settings)
        self.summarizer = DocumentSummarizer(openai_service=openai_service, settings=settings)
        self.translator = DocumentTranslator(openai_service=openai_service, settings=settings)
        self.anonymizer = DocumentAnonymizer(openai_service=openai_service, settings=settings)
        self.risk_analyzer = RiskAnalyzer(openai_service=openai_service)
        self.chat = DocumentChat(openai_service=openai_service, settings=settings)
        self.ocr_converter = OCRConverter(settings=settings)
        self.lawsuit_analyzer = LawsuitAnalyzer(openai_service=openai_service)

        self._operations: Dict[str, Dict[str, Any]] = {
            "summarize": {
                "emoji": "üìë",
                "name": "–ö—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "description": "–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∫–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã",
                "formats": ["TXT", "JSON"],
                "processor": self.summarizer,
            },
            "analyze_risks": {
                "emoji": "‚ö†Ô∏è",
                "name": "–ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤",
                "description": "–í—ã—è–≤–ª–µ–Ω–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π",
                "formats": ["TXT", "JSON"],
                "processor": self.risk_analyzer,
            },
            "lawsuit_analysis": {
                "emoji": "‚öñÔ∏è",
                "name": "–ê–Ω–∞–ª–∏–∑ –∏—Å–∫–æ–≤–æ–≥–æ –∑–∞—è–≤–ª–µ–Ω–∏—è",
                "description": "–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, –ø—Ä–∞–≤–æ–≤—É—é –ø–æ–∑–∏—Ü–∏—é –∏ —Ä–∏—Å–∫–∏ –æ—Ç–∫–∞–∑–∞",
                "formats": ["DOCX"],
                "processor": self.lawsuit_analyzer,
            },
            # "chat": {
            #     "emoji": "üí¨",
            #     "name": "–ß–∞—Ç —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º",
            #     "description": "–ó–∞–¥–∞–≤–∞–π—Ç–µ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É",
            #     "formats": ["interactive"],
            #     "processor": self.chat,
            # },
            "anonymize": {
                "emoji": "üï∂Ô∏è",
                "name": "–ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è",
                "description": "–£–¥–∞–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "formats": ["TXT", "JSON"],
                "processor": self.anonymizer,
            },
            # "translate": {
            #     "emoji": "üåê",
            #     "name": "–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞",
            #     "description": "–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —è–∑—ã–∫",
            #     "formats": ["TXT"],
            #     "processor": self.translator,
            # },
            "ocr": {
                "emoji": "üì∑",
                "name": "–†–∞—Å–ø–æ–∑–Ω–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞",
                "description": "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –∏ PDF",
                "formats": ["TXT"],
                "processor": self.ocr_converter,
            },
        }

        self._user_chat_sessions: Dict[int, Dict[str, Any]] = {}
        self._operations_cache_data: Dict[str, Dict[str, Any]] | None = None
        self._operations_cache_view: Mapping[str, Dict[str, Any]] | None = None

    # ------------------------------------------------------------------ API ---

    def _build_supported_operations(self) -> Dict[str, Dict[str, Any]]:
        operations: Dict[str, Dict[str, Any]] = {}
        for key, meta in self._operations.items():
            processor = meta.get("processor")
            upload_formats: List[str] = []
            if processor is not None:
                supported = getattr(processor, "supported_formats", None)
                if supported:
                    upload_formats = [fmt.lstrip(".").upper() for fmt in supported]

            descriptor: Dict[str, Any] = {
                "emoji": meta.get("emoji"),
                "name": meta.get("name"),
                "description": meta.get("description"),
                "formats": list(meta.get("formats", [])),
            }
            if upload_formats:
                descriptor["upload_formats"] = upload_formats
            operations[key] = descriptor
        return operations

    def get_supported_operations(self) -> Mapping[str, Dict[str, Any]]:
        if self._operations_cache_view is None:
            data = self._build_supported_operations()
            self._operations_cache_data = data
            self._operations_cache_view = MappingProxyType(data)
        return self._operations_cache_view

    def get_operation_info(self, operation: str) -> Dict[str, Any] | None:
        info = self.get_supported_operations().get(operation)
        if info is None:
            return None
        return dict(info)

    async def process_document(
        self,
        *,
        user_id: int,
        file_content: bytes,
        original_name: str,
        mime_type: str,
        operation: str,
        progress_callback: Callable[[dict[str, Any]], Awaitable[None]] | None = None,
        **options: Any,
    ) -> DocumentResult:
        meta = self._operations.get(operation)
        if not meta:
            return DocumentResult.error_result("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è", "UNKNOWN_OPERATION")

        try:
            doc_info = await self.storage.save_document(
                user_id=user_id,
                file_content=file_content,
                original_name=original_name,
                mime_type=mime_type,
            )
        except ProcessingError as exc:
            logger.warning("Failed to store document: %s", exc)
            return DocumentResult.error_result(exc.message, exc.error_code)
        except Exception as exc:  # noqa: BLE001
            logger.exception("Unexpected error while storing document")
            return DocumentResult.error_result(str(exc), "STORAGE_ERROR")

        if operation == "chat":
            return await self._handle_chat_upload(user_id, doc_info, options)

        processor = meta.get("processor")
        try:
            safe_kwargs = dict(options)
            if progress_callback is not None:
                safe_kwargs["progress_callback"] = progress_callback
            result = await processor.safe_process(doc_info.file_path, **safe_kwargs)
        except ProcessingError as exc:
            self._safe_unlink(doc_info.file_path)
            logger.warning("Processor error (%s): %s", operation, exc)
            return DocumentResult.error_result(exc.message, exc.error_code)
        except Exception as exc:  # noqa: BLE001
            self._safe_unlink(doc_info.file_path)
            logger.exception("Unhandled error during %s", operation)
            return DocumentResult.error_result(str(exc), "PROCESSING_ERROR")

        result.data.setdefault("document_info", asdict(doc_info))
        if result.success:
            try:
                exports = await self._prepare_exports(operation, result, doc_info)
            except ProcessingError as exc:
                logger.warning("Failed to prepare exports for %s: %s", operation, exc)
                result = DocumentResult.error_result(exc.message, exc.error_code or "EXPORT_ERROR")
            except Exception as exc:  # noqa: BLE001
                logger.exception("Unexpected error while preparing exports for %s", operation)
                result = DocumentResult.error_result(
                    "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ñ–∞–π–ª –æ—Ç—á–µ—Ç–∞", "EXPORT_ERROR"
                )
            else:
                if exports:
                    existing = list(result.data.get("exports") or [])
                    result.data["exports"] = existing + exports
        if not result.success:
            result.data.setdefault("exports", [])

        self._safe_unlink(doc_info.file_path)
        return result

    def format_result_for_telegram(self, result: DocumentResult, operation: str) -> str:
        formatter = {
            "summarize": self._format_summary_result,
            "translate": self._format_translation_result,
            "anonymize": self._format_anonymize_result,
            "analyze_risks": self._format_risk_result,
            "lawsuit_analysis": self._format_lawsuit_result,
            "chat": self._format_chat_loaded,
            "ocr": self._format_ocr_result,
        }.get(operation, self._format_generic_result)
        return formatter(result.data, result.message)

    def format_chat_answer_for_telegram(self, result: DocumentResult) -> str:
        data = result.data
        answer = html_escape(data.get("answer", ""))
        confidence = float(data.get("confidence", 0.0))
        citations = data.get("citations", [])
        lines = [
            "<b>–û—Ç–≤–µ—Ç –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É</b>",
            answer or "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞",
            "",
            f"–î–æ–≤–µ—Ä–∏–µ –º–æ–¥–µ–ª–∏: {confidence:.0%}",
        ]
        if citations:
            lines.append("<b>–¶–∏—Ç–∞—Ç—ã:</b>")
            for item in citations[:5]:
                snippet = html_escape(str(item.get("snippet", "")))
                idx = item.get("chunk_index")
                lines.append(f"‚Ä¢ [chunk #{idx}] {snippet}")
        return "\n".join(lines)

    async def answer_chat_question(self, user_id: int, question: str) -> DocumentResult:
        session = self._user_chat_sessions.get(user_id)
        if not session:
            raise ProcessingError("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è —á–∞—Ç–∞", "CHAT_NOT_INITIALIZED")
        document_id = session["document_id"]
        return await self.chat.answer_question(document_id, question)

    def end_chat_session(self, user_id: int) -> bool:
        session = self._user_chat_sessions.pop(user_id, None)
        if session:
            self.chat.unload_document(session.get("document_id"))
            return True
        return False


    # -------------------------------------------------------------- helpers ---

    def _init_storage(self, settings: AppSettings) -> DocumentStorage:
        storage_root = settings.get_str("DOCUMENTS_STORAGE_PATH", None) or "data/documents"
        uploader = self._build_uploader(settings)
        return DocumentStorage(
            storage_path=storage_root,
            max_user_quota_mb=settings.document_storage_quota_mb,
            cleanup_max_age_hours=settings.document_cleanup_hours,
            cleanup_interval_seconds=settings.document_cleanup_interval_seconds,
            artifact_uploader=uploader,
        )

    def _build_uploader(self, settings: AppSettings) -> ArtifactUploader | None:
        bucket = settings.documents_s3_bucket
        if not bucket:
            return None
        return S3ArtifactUploader(
            bucket=bucket,
            prefix=settings.documents_s3_prefix,
            region_name=settings.documents_s3_region,
            endpoint_url=settings.documents_s3_endpoint,
            public_base_url=settings.documents_s3_public_url,
            acl=settings.documents_s3_acl,
        )

    async def _handle_chat_upload(
        self,
        user_id: int,
        doc_info: DocumentInfo,
        options: Dict[str, Any],
    ) -> DocumentResult:
        previous = self._user_chat_sessions.pop(user_id, None)
        if previous:
            self.chat.unload_document(previous.get("document_id"))
        try:
            document_id = await self.chat.load_document(doc_info.file_path)
        finally:
            self._safe_unlink(doc_info.file_path)
        session_payload = {
            "document_id": document_id,
            "info": asdict(doc_info),
        }
        self._user_chat_sessions[user_id] = session_payload
        data = {
            "document_id": document_id,
            "document_info": self.chat.get_document_info(document_id),
            "message": "–î–æ–∫—É–º–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —á–∞—Ç—É. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /askdoc –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ /enddoc –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.",
        }
        return DocumentResult.success_result(data=data, message="–î–æ–∫—É–º–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —á–∞—Ç—É")

    async def _prepare_exports(
        self,
        operation: str,
        result: DocumentResult,
        doc_info: DocumentInfo,
    ) -> List[Dict[str, Any]]:
        exports: List[Dict[str, Any]] = []
        base_name = Path(doc_info.original_name or "document").stem or "document"

        if operation == "summarize":
            summary_block = ((result.data.get("summary") or {}).get("content") or "")
            structured = (result.data.get("summary") or {}).get("structured") or {}
            docx_path = await self._build_docx_summary(base_name, summary_block, structured)
            exports.append({"path": str(docx_path), "format": "docx", "label": "–í—ã–∂–∏–º–∫–∞ (DOCX)"})

        elif operation == "translate":
            translated = (result.data.get("translated_text") or "").strip()
            if translated:
                path = await self._write_export(base_name, "translation", translated, ".txt")
                exports.append({"path": str(path), "format": "txt", "label": "–ü–µ—Ä–µ–≤–æ–¥"})

        elif operation == "anonymize":
            anonymized = (result.data.get("anonymized_text") or "").strip()
            if anonymized:
                docx_path = await self._build_docx_anonymized(base_name, anonymized)
                exports.append({"path": str(docx_path), "format": "docx", "label": "–ê–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"})

        elif operation == "analyze_risks":
            docx_path = await self._build_docx_risk(base_name, result.data or {})
            exports.append({"path": str(docx_path), "format": "docx", "label": "–ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ (DOCX)"})

        elif operation == "lawsuit_analysis":
            analysis = result.data.get("analysis") or {}
            markdown = self._build_lawsuit_markdown(analysis).strip()
            if not markdown:
                raise ProcessingError(
                    "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è DOCX-–æ—Ç—á–µ—Ç–∞",
                    "EMPTY_DOCX_CONTENT",
                )
            docx_path = self._build_human_friendly_temp_path(base_name, "–∞–Ω–∞–ª–∏–∑ –∏—Å–∫–∞", ".docx")
            try:
                await asyncio.to_thread(build_docx_from_markdown, markdown, str(docx_path))
            except DocumentDraftingError as exc:
                raise ProcessingError(str(exc), "DOCX_EXPORT_ERROR") from exc
            except Exception as exc:  # noqa: BLE001
                raise ProcessingError(
                    "–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ DOCX-–æ—Ç—á–µ—Ç–∞",
                    "DOCX_EXPORT_ERROR",
                ) from exc
            exports.append({"path": str(docx_path), "format": "docx", "label": "–ê–Ω–∞–ª–∏–∑ (DOCX)"})

        elif operation == "ocr":
            recognized = (result.data.get("recognized_text") or "").strip()
            if recognized:
                docx_path = await self._build_docx_ocr(base_name, recognized)
                exports.append({"path": str(docx_path), "format": "docx", "label": "–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (DOCX)"})

        return exports

    @staticmethod
    def _sanitize_filename_component(value: str, fallback: str | None = None) -> str:
        sanitized = _FILENAME_SANITIZE_RE.sub(" ", (value or ""))
        sanitized = _FILENAME_WHITESPACE_RE.sub(" ", sanitized).strip(" .-_")
        if not sanitized and fallback is not None:
            sanitized = fallback
        return sanitized

    def _build_human_friendly_temp_path(self, base: str, suffix: str, extension: str) -> Path:
        ext = extension if extension.startswith(".") else f".{extension}"
        base_clean = self._sanitize_filename_component(base, fallback="–î–æ–∫—É–º–µ–Ω—Ç")
        suffix_clean = self._sanitize_filename_component(suffix, fallback="")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        name_parts = [base_clean]
        if suffix_clean:
            name_parts.append(suffix_clean)
        name_core = " - ".join(name_parts)
        filename = f"{name_core} ({timestamp}){ext}"
        temp_dir = Path(tempfile.gettempdir())
        candidate = temp_dir / filename
        counter = 1
        while candidate.exists():
            candidate = temp_dir / f"{name_core} ({timestamp}_{counter}){ext}"
            counter += 1
        return candidate

    async def _write_export(self, base: str, suffix: str, content: str, extension: str) -> Path:
        target = Path(tempfile.gettempdir()) / f"aivan_{base}_{suffix}_{uuid.uuid4().hex}{extension}"
        return await write_text_async(target, content)

    async def _build_docx_from_markdown_safe(self, base_name: str, suffix: str, markdown: str) -> Path:
        docx_path = self._build_human_friendly_temp_path(base_name, suffix, ".docx")
        content = (markdown or "").strip()
        if not content:
            content = f"# {suffix or '–î–æ–∫—É–º–µ–Ω—Ç'}\n\n(–¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç)"
        try:
            await asyncio.to_thread(build_docx_from_markdown, content, str(docx_path))
        except DocumentDraftingError as exc:
            raise ProcessingError(str(exc), "DOCX_EXPORT_ERROR") from exc
        except Exception as exc:  # noqa: BLE001
            raise ProcessingError("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ DOCX-—Ñ–∞–π–ª–∞", "DOCX_EXPORT_ERROR") from exc
        return docx_path

    @staticmethod
    def _safe_unlink(path: str | Path) -> None:
        try:
            Path(path).unlink(missing_ok=True)
        except Exception:
            pass

    async def _build_docx_anonymized(self, base_name: str, anonymized_text: str) -> Path:
        markdown = self._anonymized_to_markdown(anonymized_text)
        return await self._build_docx_from_markdown_safe(base_name, "–∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è", markdown)

    async def _build_docx_summary(
        self,
        base_name: str,
        summary_text: str,
        structured: Mapping[str, Any] | Dict[str, Any] | None,
    ) -> Path:
        sections: list[str] = ["# –ö—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞"]
        summary_clean = (summary_text or "").strip()
        if summary_clean:
            sections.append(summary_clean)

        def _add_list(title: str, items: Any) -> None:
            values = [str(item).strip() for item in (items or []) if str(item).strip()]
            if not values:
                return
            sections.append(f"## {title}")
            sections.extend(f"- {value}" for value in values)

        data = dict(structured or {})
        _add_list("–ö–ª—é—á–µ–≤—ã–µ –ø—É–Ω–∫—Ç—ã", data.get("key_points"))
        _add_list("–°—Ä–æ–∫–∏", data.get("deadlines"))
        _add_list("–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", data.get("penalties"))
        _add_list("–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –¥–µ–π—Å—Ç–≤–∏—è", data.get("actions"))

        markdown = "\n\n".join(sections)
        return await self._build_docx_from_markdown_safe(base_name, "–≤—ã–∂–∏–º–∫–∞", markdown)

    async def _build_docx_risk(self, base_name: str, risk_data: Mapping[str, Any]) -> Path:
        data = dict(risk_data or {})
        sections: list[str] = ["# –ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤"]
        overall = str(data.get("overall_risk_level") or "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω")
        sections.append(f"**–û–±—â–∏–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞:** {overall}")

        summary = (data.get("ai_analysis") or {}).get("summary") or ""
        if summary:
            sections.append("")
            sections.append(summary.strip())

        def _format_risks(title: str, items: Any) -> None:
            risks = []
            for item in items or []:
                entry = item or {}
                desc = str(entry.get("description") or entry.get("note") or entry.get("text") or "").strip()
                level = str((item or {}).get("risk_level") or "").strip()
                if desc:
                    prefix = f"[{level.upper()}] " if level else ""
                    risks.append(f"- {prefix}{desc}")
            if risks:
                sections.append(f"## {title}")
                sections.extend(risks)

        _format_risks("–ü–∞—Ç—Ç–µ—Ä–Ω—ã –∏ –ø—Ä–∞–≤–∏–ª–∞", data.get("pattern_risks"))
        ai_risks = ((data.get("ai_analysis") or {}).get("risks")) or []
        _format_risks("–í—ã—è–≤–ª–µ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏", ai_risks)

        recommendations = [str(item).strip() for item in data.get("recommendations") or [] if str(item).strip()]
        if recommendations:
            sections.append("## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
            sections.extend(f"- {text}" for text in recommendations)

        compliance = (data.get("legal_compliance") or {}).get("violations") or []
        _format_risks("–ù–∞—Ä—É—à–µ–Ω–∏—è", compliance)

        markdown = "\n\n".join(sections)
        return await self._build_docx_from_markdown_safe(base_name, "–∞–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤", markdown)

    async def _build_docx_ocr(self, base_name: str, recognized_text: str) -> Path:
        text = (recognized_text or "").strip()
        markdown = "# –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n\n" + (text if text else "(–¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç)")
        return await self._build_docx_from_markdown_safe(base_name, "—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", markdown)

    @staticmethod
    def _anonymized_to_markdown(text: str) -> str:
        if not text.strip():
            return ""
        paragraphs = text.split("\n\n")
        pieces = []
        for para in paragraphs:
            stripped = para.strip()
            if not stripped:
                continue
            if stripped.startswith("‚Ä¢ ") or stripped.startswith("- "):
                lines = [line.strip() for line in stripped.split("\n") if line.strip()]
                pieces.extend(lines)
            else:
                pieces.append(stripped)
        return "\n\n".join(pieces)

    # ----------------------------------- formatters -----------------------------------

    def _format_summary_result(self, data: Dict[str, Any], message: str) -> str:
        summary_payload = data.get("summary") or {}
        summary_block = summary_payload.get("content") or ""
        structured = summary_payload.get("structured") or {}
        metadata = data.get("metadata") or {}
        processing_info = data.get("processing_info") or {}

        key_points = structured.get("key_points") or []
        deadlines = structured.get("deadlines") or []
        penalties = structured.get("penalties") or []
        checklist = structured.get("actions") or []

        doc_info = data.get("document_info") or {}
        original_name = str(doc_info.get("original_name") or "").strip()
        title = Path(original_name).stem if original_name else ""
        title = title.replace("_", " ").replace("-", " ").strip()
        if not title:
            title = "–ö—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"

        divider_html = "<code>" + ("‚îÄ" * 30) + "</code>"
        title_html = html_escape(title)

        lines: list[str] = [
            f"<b>üìÑ {title_html}</b>",
            divider_html,
            "",
            "<b>‚ú® –ö—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞ –≥–æ—Ç–æ–≤–∞!</b>",
            "üìé <b>–§–æ—Ä–º–∞—Ç:</b> DOCX",
        ]

        stats: list[str] = []
        chunks_processed = processing_info.get("chunks_processed")
        if chunks_processed:
            stats.append(f"chunks: {chunks_processed}")

        detail_level = str(data.get("detail_level") or "").lower()
        detail_map = {"detailed": "–¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è: –ø–æ–¥—Ä–æ–±–Ω–∞—è", "brief": "–¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è: –∫—Ä–∞—Ç–∫–∞—è"}
        if detail_level in detail_map:
            stats.append(detail_map[detail_level])

        language_display = metadata.get("language_display") or metadata.get("language")
        if language_display:
            stats.append(f"—è–∑—ã–∫: {language_display}")

        key_count = len(key_points)
        if key_count:
            stats.append(f"–∫–ª—é—á–µ–≤—ã—Ö –ø—É–Ω–∫—Ç–æ–≤: {key_count}")
        deadlines_count = len(deadlines)
        if deadlines_count:
            stats.append(f"—Å—Ä–æ–∫–æ–≤: {deadlines_count}")
        penalties_count = len(penalties)
        if penalties_count:
            stats.append(f"—Å–∞–Ω–∫—Ü–∏–π: {penalties_count}")

        if stats:
            stats_text = ", ".join(html_escape(item) for item in stats)
            lines.extend(["", f"üìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {stats_text}"])

        structured_summary = str(structured.get("summary") or "").strip()
        preview_source = structured_summary or summary_block
        preview_flat = re.sub(r"\s+", " ", str(preview_source)).strip()
        if preview_flat:
            if len(preview_flat) > 280:
                preview_flat = preview_flat[:277].rstrip() + "..."
            lines.extend(["", f"<b>üìù –ö—Ä–∞—Ç–∫–æ:</b> {html_escape(preview_flat)}"])

        def append_section(title: str, icon: str, items: list[Any], limit: int = 5) -> None:
            if not items:
                return
            lines.append("")
            lines.append(f"<b>{icon} {title}</b>")
            for entry in items[:limit]:
                lines.append(f"‚Ä¢ {html_escape(str(entry))}")

        append_section("–û—Å–Ω–æ–≤–Ω–æ–µ", "üìå", key_points, limit=6)
        append_section("–°—Ä–æ–∫–∏", "‚è∞", deadlines, limit=5)
        append_section("–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "‚öñÔ∏è", penalties, limit=5)
        append_section("–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –¥–µ–π—Å—Ç–≤–∏—è", "‚úÖ", checklist, limit=6)

        lines.extend(["", "<i>üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–Ω–µ—Å–∏—Ç–µ –ø—Ä–∞–≤–∫–∏.</i>"])

        return "\n".join(lines)

    def _format_translation_result(self, data: Dict[str, Any], message: str) -> str:
        translated = data.get("translated_text") or ""
        src = data.get("source_language") or "?"
        tgt = data.get("target_language") or "?"
        lines = [
            "<b>–ü–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞</b>",
            f"–Ø–∑—ã–∫–æ–≤–∞—è –ø–∞—Ä–∞: {html_escape(str(src))} ‚Üí {html_escape(str(tgt))}",
            "",
            html_escape(translated)[:3500],
        ]
        return "\n".join(lines)

    def _format_anonymize_result(self, data: Dict[str, Any], message: str) -> str:
        lines: list[str] = [
            "<b>‚ú® –î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –æ–±–µ–∑–ª–∏—á–µ–Ω!</b>",
            "üìé <b>–§–æ—Ä–º–∞—Ç:</b> DOCX",
        ]

        report = data.get("anonymization_report") or {}
        counters = report.get("statistics") or report.get("counters") or {}
        total_masked = report.get("processed_items")
        if total_masked is None:
            total_masked = report.get("total_matches")
        if total_masked is None and counters:
            try:
                total_masked = sum(int(v) for v in counters.values())
            except Exception:  # noqa: BLE001
                total_masked = None

        try:
            total_int = int(total_masked) if total_masked is not None else None
        except (TypeError, ValueError):
            total_int = None

        if total_int is not None:
            lines.extend(["", f"üõ°Ô∏è –û–±–µ–∑–ª–∏—á–µ–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {total_int}"])

        replacements_map = data.get("anonymization_map") or {}
        processed_items = report.get("processed_items") or []
        if replacements_map and processed_items:
            type_labels = report.get("type_labels") or {}
            seen_pairs: set[tuple[str, str]] = set()
            display_rows: list[str] = []
            for item in processed_items:
                original = str(item.get("value") or "").strip()
                if not original:
                    continue
                replacement_raw = replacements_map.get(original)
                if replacement_raw is None:
                    continue

                original_clean = re.sub(r"\s+", " ", original).strip()
                if len(original_clean) > 60:
                    original_clean = original_clean[:57].rstrip() + "..."

                replacement_display = replacement_raw.strip()
                if not replacement_display:
                    replacement_display = "[—É–¥–∞–ª–µ–Ω–æ]"

                label = str(item.get("label") or "").strip()
                if not label:
                    item_type = str(item.get("type") or "").strip()
                    label = str(type_labels.get(item_type, "") or "").strip()
                label_display = label or "–°—É—â–Ω–æ—Å—Ç—å"

                key = (original.strip().lower(), replacement_display.lower())
                if key in seen_pairs:
                    continue
                seen_pairs.add(key)

                display_rows.append(
                    f"‚Ä¢ {html_escape(original_clean)} ‚Üí {html_escape(replacement_display)}"
                    f" ({html_escape(label_display)})"
                )
                if len(display_rows) >= 5:
                    break

            if display_rows:
                lines.extend(["", "<b>üîÅ –ó–∞–º–µ–Ω—ã:</b>"])
                lines.extend(display_rows)


        preview_source = str(data.get("anonymized_text") or "")
        preview_clean = re.sub(r"\s+", " ", preview_source).strip()
        if preview_clean:
            if len(preview_clean) > 280:
                preview_clean = preview_clean[:277].rstrip() + "..."
            lines.extend(["", f"<b>üìù –ö—Ä–∞—Ç–∫–æ:</b> {html_escape(preview_clean)}"])

        lines.extend(["", "<i>üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–Ω–µ—Å–∏—Ç–µ –ø—Ä–∞–≤–∫–∏.</i>"])

        raw_notes = report.get("notes") or []
        meaningful_notes: list[str] = []
        for note in raw_notes:
            note_text = str(note or "").strip()
            if not note_text:
                continue
            if "–∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞" in note_text.lower():
                continue
            meaningful_notes.append(note_text)
        for note_text in meaningful_notes[:3]:
            lines.extend(["", f"<i>{html_escape(note_text)}</i>"])

        return "\n".join(lines).strip()

    def _format_risk_result(self, data: Dict[str, Any], message: str) -> str:
        overall = data.get("overall_risk_level") or "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"
        recommendations = data.get("recommendations") or []
        pattern_risks = data.get("pattern_risks") or []
        lines = [
            "<b>–ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤</b>",
            f"–û–±—â–∏–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞: {html_escape(str(overall))}",
        ]
        if pattern_risks:
            lines.append("")
            lines.append("<b>–í–∞–∂–Ω—ã–µ –Ω–∞—Ö–æ–¥–∫–∏:</b>")
            for item in pattern_risks[:5]:
                desc = item.get("description") or ""
                level = item.get("risk_level") or ""
                lines.append(f"‚Ä¢ {html_escape(str(level))}: {html_escape(desc)}")
        if recommendations:
            lines.append("")
            lines.append("<b>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:</b>")
            for rec in recommendations[:6]:
                lines.append(f"‚Ä¢ {html_escape(str(rec))}")
        return "\n".join(lines)

    def _format_lawsuit_result(self, data: Dict[str, Any], message: str) -> str:
        analysis: dict[str, Any] = data.get("analysis") or {}
        doc_info = data.get("document_info") or {}
        original_name = str(doc_info.get("original_name") or "").strip()
        title = Path(original_name).stem if original_name else ""
        title = title.replace("_", " ").replace("-", " ").strip()
        if not title:
            title = "–ê–Ω–∞–ª–∏–∑ –∏—Å–∫–æ–≤–æ–≥–æ –∑–∞—è–≤–ª–µ–Ω–∏—è"

        divider = "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
        title_html = html_escape(title)
        divider_html = f"<code>{divider}</code>"
        lines: list[str] = [
            f"<b>üìÑ {title_html}</b>",
            divider_html,
            "",
            "<b>‚ú® –î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!</b>",
            "üìé <b>–§–æ—Ä–º–∞—Ç:</b> DOCX",
        ]

        summary = str(analysis.get("summary") or "").strip()
        if summary:
            summary_clean = re.sub(r"\s+", " ", summary)
            if len(summary_clean) > 280:
                summary_clean = summary_clean[:277].rstrip() + "..."
            lines.extend(["", f"<b>üìù –ö—Ä–∞—Ç–∫–æ:</b> {html_escape(summary_clean)}"])

        lines.extend(["", "<i>üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–Ω–µ—Å–∏—Ç–µ –ø—Ä–∞–≤–∫–∏.</i>"])

        if data.get("truncated"):
            lines.extend(["", "<i>‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω –ø–æ —É—Å–µ—á—ë–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞.</i>"])

        report = data.get("anonymization_report") or {}
        engine = report.get("engine")
        notes = report.get("notes") or []
        if engine == "openai":
            lines.append("")
            lines.append("<i>‚úîÔ∏è –ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞.</i>")
        elif engine == "fallback":
            lines.append("")
            lines.append("<i>‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Ä–µ–∂–∏–º –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏–∏.</i>")
        if engine == "pattern":
            stats = report.get("statistics") or {}
            total_masked = sum(stats.values())
            lines.append("")
            lines.append(f"<i>üîê –û–±–µ–∑–ª–∏—á–µ–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {total_masked}</i>")
        for note in notes:
            if not note:
                continue
            lines.append("")
            lines.append(f"<i>{html_escape(str(note))}</i>")

        return "\n".join(lines).strip()

    def _format_chat_loaded(self, data: Dict[str, Any], message: str) -> str:
        info = data.get("document_info") or {}
        metadata = info.get("metadata") or {}
        doc_id = data.get("document_id")
        top_keywords: List[str] = []
        if doc_id and doc_id in self.chat.loaded_documents:
            try:
                source_text = self.chat.loaded_documents[doc_id]["text"]
                top_keywords = TextProcessor.top_keywords(source_text, limit=5)
            except Exception:
                top_keywords = []
        lines = [
            "<b>–î–æ–∫—É–º–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —á–∞—Ç—É</b>",
            "–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º –≤ —ç—Ç–æ–º –¥–∏–∞–ª–æ–≥–µ. –î–ª—è –≤—ã—Ö–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ /cancel",
            "",
            f"–û–±—ä–µ–º: {metadata.get('words', 0)} —Å–ª–æ–≤",
            f"–ß–∏—Å–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {metadata.get('sentences', 0)}",
        ]
        if top_keywords:
            lines.append("")
            lines.append("<b>–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:</b> " + ", ".join(top_keywords))
        return "\n".join(lines)

    def _format_ocr_result(self, data: Dict[str, Any], message: str) -> str:
        text = (data.get("recognized_text") or "")[:3500]
        confidence = float(data.get("confidence_score", 0.0))
        lines = [
            "<b>–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç</b>",
            f"–¢–æ—á–Ω–æ—Å—Ç—å: {confidence:.1f}%",
            "",
            html_escape(text),
        ]
        return "\n".join(lines)

    def _format_generic_result(self, data: Dict[str, Any], message: str) -> str:
        return html_escape(message or "–ì–æ—Ç–æ–≤–æ")

    @staticmethod
    def _build_lawsuit_markdown(analysis: Dict[str, Any]) -> str:
        lines = ["# –ê–Ω–∞–ª–∏–∑ –∏—Å–∫–æ–≤–æ–≥–æ –∑–∞—è–≤–ª–µ–Ω–∏—è", ""]

        summary = str(analysis.get("summary") or "").strip()
        if summary:
            lines.extend(["## ‚öñÔ∏è –†–µ–∑—é–º–µ", summary, ""])

        parties = analysis.get("parties") or {}
        party_lines: list[str] = []
        if parties.get("plaintiff"):
            party_lines.append(f"- –ò—Å—Ç–µ—Ü: {parties['plaintiff']}")
        if parties.get("defendant"):
            party_lines.append(f"- –û—Ç–≤–µ—Ç—á–∏–∫: {parties['defendant']}")
        for item in parties.get("other") or []:
            text = str(item or "").strip()
            if text:
                party_lines.append(f"- –£—á–∞—Å—Ç–Ω–∏–∫: {text}")
        if party_lines:
            lines.extend(["## üë• –°—Ç–æ—Ä–æ–Ω—ã", *party_lines, ""])

        def append_block(key: str, values: Any) -> None:
            cleaned = [str(value or "").strip() for value in (values or []) if str(value or "").strip()]
            if not cleaned:
                return
            icon, title = _LAWSUIT_SECTION_META[key]
            lines.append(f"## {icon} {title}")
            for entry in cleaned:
                lines.append(f"- {entry}")
            lines.append("")

        for section_key in (
            "demands",
            "legal_basis",
            "evidence",
            "strengths",
            "risks",
            "missing_elements",
            "recommendations",
            "procedural_notes",
        ):
            append_block(section_key, analysis.get(section_key))

        confidence = str(analysis.get("confidence") or "").strip()
        if confidence:
            lines.extend(["", f"_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞: {confidence}_"])

        return "\n".join(lines).strip()
