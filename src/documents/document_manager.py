"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ–º–∏ –º–æ–¥—É–ª—è–º–∏ —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
"""

from __future__ import annotations
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Union, Optional
import logging

from .base import DocumentStorage, ProcessingError, DocumentResult
from .summarizer import DocumentSummarizer
from .risk_analyzer import RiskAnalyzer
from .document_chat import DocumentChat
from .anonymizer import DocumentAnonymizer
from .translator import DocumentTranslator
from .ocr_converter import OCRConverter

logger = logging.getLogger(__name__)

class DocumentManager:
    """–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"""

    def __init__(self, openai_service=None, storage_path: str = "data/documents"):
        self.openai_service = openai_service
        self.storage = DocumentStorage(storage_path)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π
        self.summarizer = DocumentSummarizer(openai_service)
        self.risk_analyzer = RiskAnalyzer(openai_service)
        self.document_chat = DocumentChat(openai_service)
        self.anonymizer = DocumentAnonymizer()
        self.translator = DocumentTranslator(openai_service)
        self.ocr_converter = OCRConverter()

    async def process_document(
        self,
        user_id: int,
        file_content: bytes,
        original_name: str,
        mime_type: str,
        operation: str,
        **kwargs
    ) -> DocumentResult:
        """
        –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º

        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            file_content: —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
            original_name: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            mime_type: MIME —Ç–∏–ø —Ñ–∞–π–ª–∞
            operation: —Ç–∏–ø –æ–ø–µ—Ä–∞—Ü–∏–∏ ("summarize", "analyze_risks", "chat", "anonymize", "translate", "ocr")
            **kwargs: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–∏
        """

        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
            document_info = await self.storage.save_document(
                user_id, file_content, original_name, mime_type
            )

            # –í—ã–±–∏—Ä–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
            processor = self._get_processor(operation)
            if not processor:
                raise ProcessingError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è: {operation}", "UNKNOWN_OPERATION")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç {original_name} –æ–ø–µ—Ä–∞—Ü–∏–µ–π {operation} –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            result = await processor.safe_process(document_info.file_path, **kwargs)

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if result.success:
                result.data["document_info"] = {
                    "original_name": document_info.original_name,
                    "file_size": document_info.size,
                    "upload_time": document_info.upload_time.isoformat(),
                    "user_id": user_id
                }

            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            self.storage.cleanup_old_files(user_id, max_age_hours=24)

            return result

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {original_name}: {e}")
            raise ProcessingError(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}", "PROCESSING_ERROR")

    def _get_processor(self, operation: str):
        """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏"""
        processors = {
            "summarize": self.summarizer,
            "analyze_risks": self.risk_analyzer,
            "chat": self.document_chat,
            "anonymize": self.anonymizer,
            "translate": self.translator,
            "ocr": self.ocr_converter
        }
        return processors.get(operation)

    async def chat_with_document(self, user_id: int, document_id: str, question: str) -> Dict[str, Any]:
        """–ß–∞—Ç —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–º"""
        try:
            return await self.document_chat.chat_with_document(document_id, question)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º {document_id}: {e}")
            raise ProcessingError(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞: {str(e)}", "CHAT_ERROR")

    def get_supported_operations(self) -> Dict[str, Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"""
        return {
            "summarize": {
                "name": "–°–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è",
                "description": "–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–π –≤—ã–∂–∏–º–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∫–ª—é—á–µ–≤—ã–º–∏ –ø–æ–ª–æ–∂–µ–Ω–∏—è–º–∏",
                "emoji": "üìã",
                "formats": [".pdf", ".docx", ".doc", ".txt"],
                "parameters": ["detail_level"]
            },
            "analyze_risks": {
                "name": "–ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤",
                "description": "–í—ã—è–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–æ–≤ –∏ –ø—Ä–æ–±–ª–µ–º –≤ –¥–æ–≥–æ–≤–æ—Ä–∞—Ö –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö",
                "emoji": "‚ö†Ô∏è",
                "formats": [".pdf", ".docx", ".doc", ".txt"],
                "parameters": ["custom_criteria"]
            },
            "chat": {
                "name": "–ß–∞—Ç —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º",
                "description": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã-–æ—Ç–≤–µ—Ç—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É",
                "emoji": "üí¨",
                "formats": [".pdf", ".docx", ".doc", ".txt"],
                "parameters": []
            },
            "anonymize": {
                "name": "–û–±–µ–∑–ª–∏—á–∏–≤–∞–Ω–∏–µ",
                "description": "–£–¥–∞–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "emoji": "üîí",
                "formats": [".pdf", ".docx", ".doc", ".txt"],
                "parameters": ["anonymization_mode"]
            },
            "translate": {
                "name": "–ü–µ—Ä–µ–≤–æ–¥",
                "description": "–ü–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ –¥—Ä—É–≥–∏–µ —è–∑—ã–∫–∏",
                "emoji": "üåç",
                "formats": [".pdf", ".docx", ".doc", ".txt"],
                "parameters": ["source_lang", "target_lang"]
            },
            "ocr": {
                "name": "OCR —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ",
                "description": "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
                "emoji": "üëÅÔ∏è",
                "formats": [".pdf", ".jpg", ".jpeg", ".png", ".tiff", ".tif", ".bmp"],
                "parameters": ["output_format"]
            }
        }

    def get_operation_info(self, operation: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–ø–µ—Ä–∞—Ü–∏–∏"""
        return self.get_supported_operations().get(operation)

    def format_result_for_telegram(self, result: DocumentResult, operation: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram"""
        if not result.success:
            return f"‚ùå **–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏**\n\n{result.message}"

        operation_info = self.get_operation_info(operation)
        emoji = operation_info.get("emoji", "üìÑ") if operation_info else "üìÑ"

        header = f"{emoji} **{operation_info.get('name', operation.title()) if operation_info else operation.title()}**\n"
        header += f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result.processing_time:.1f}—Å\n\n"

        if operation == "summarize":
            return self._format_summary_result(header, result.data)
        elif operation == "analyze_risks":
            return self._format_risk_analysis_result(header, result.data)
        elif operation == "chat":
            return self._format_chat_result(header, result.data)
        elif operation == "anonymize":
            return self._format_anonymize_result(header, result.data)
        elif operation == "translate":
            return self._format_translate_result(header, result.data)
        elif operation == "ocr":
            return self._format_ocr_result(header, result.data)
        else:
            return f"{header}‚úÖ {result.message}"

    def _format_summary_result(self, header: str, data: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
        summary = data.get("summary", {})
        content = summary.get("content", "–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")

        metadata = data.get("metadata", {})

        result = f"{header}**üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞:**\n"
        result += f"‚Ä¢ –°–ª–æ–≤: {metadata.get('word_count', '–Ω/–¥')}\n"
        result += f"‚Ä¢ –°–∏–º–≤–æ–ª–æ–≤: {metadata.get('char_count', '–Ω/–¥')}\n\n"

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è Telegram
        if len(content) > 3000:
            content = content[:3000] + "...\n\n*(–ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è –≤—ã–∂–∏–º–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ —Ñ–∞–π–ª–µ)*"

        result += content

        return result

    def _format_risk_analysis_result(self, header: str, data: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∏—Å–∫–æ–≤"""
        overall_risk = data.get("overall_risk_level", "–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω")

        risk_emojis = {
            "low": "üü¢",
            "medium": "üü°",
            "high": "üü†",
            "critical": "üî¥"
        }

        emoji = risk_emojis.get(overall_risk.lower(), "‚ùì")

        result = f"{header}**{emoji} –û–±—â–∏–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞: {overall_risk.upper()}**\n\n"

        pattern_risks = data.get("pattern_risks", [])
        if pattern_risks:
            result += f"**‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏: {len(pattern_risks)}**\n\n"

            for risk in pattern_risks[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
                level_emoji = risk_emojis.get(risk.get("risk_level", "medium"), "üü°")
                result += f"{level_emoji} {risk.get('description', '–ù–µ–æ–ø–∏—Å–∞–Ω–Ω—ã–π —Ä–∏—Å–∫')}\n"

            if len(pattern_risks) > 3:
                result += f"\n*...–∏ –µ—â—ë {len(pattern_risks) - 3} —Ä–∏—Å–∫–æ–≤*\n"

        ai_analysis = data.get("ai_analysis", {})
        if ai_analysis.get("analysis"):
            analysis_text = ai_analysis["analysis"]
            if len(analysis_text) > 1500:
                analysis_text = analysis_text[:1500] + "...\n\n*(–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ñ–∞–π–ª–µ)*"
            result += f"\n**ü§ñ AI-–∞–Ω–∞–ª–∏–∑:**\n{analysis_text}"

        return result

    def _format_chat_result(self, header: str, data: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —á–∞—Ç–∞"""
        question = data.get("question", "")
        answer = data.get("answer", "–û—Ç–≤–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

        result = f"**‚ùì –í–æ–ø—Ä–æ—Å:** {question}\n\n"
        result += f"**üí° –û—Ç–≤–µ—Ç:**\n{answer}\n\n"

        fragments = data.get("relevant_fragments", [])
        if fragments:
            result += f"**üìé –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:**\n"
            for i, fragment in enumerate(fragments[:2], 1):
                result += f"{i}. *{fragment.get('text', '')[:200]}...*\n"

        return result

    def _format_anonymize_result(self, header: str, data: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ–±–µ–∑–ª–∏—á–∏–≤–∞–Ω–∏—è"""
        report = data.get("anonymization_report", {})
        stats = report.get("statistics", {})

        result = f"{header}**üîí –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±–µ–∑–ª–∏—á–∏–≤–∞–Ω–∏—è:**\n\n"

        total_items = sum(stats.values())
        result += f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: **{total_items}**\n\n"

        if stats:
            result += "**–ü–æ —Ç–∏–ø–∞–º –¥–∞–Ω–Ω—ã—Ö:**\n"
            type_names = {
                "names": "üë§ –§–ò–û",
                "phones": "üìû –¢–µ–ª–µ—Ñ–æ–Ω—ã",
                "emails": "üìß Email",
                "addresses": "üè† –ê–¥—Ä–µ—Å–∞",
                "documents": "üìÑ –ù–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
                "bank_details": "üè¶ –ë–∞–Ω–∫–æ–≤—Å–∫–∏–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã"
            }

            for data_type, count in stats.items():
                if count > 0:
                    name = type_names.get(data_type, data_type)
                    result += f"‚Ä¢ {name}: {count}\n"

        result += f"\n‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –ø–µ—Ä–µ–¥–∞—á–µ"

        return result

    def _format_translate_result(self, header: str, data: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–µ—Ä–µ–≤–æ–¥–∞"""
        source_lang = data.get("source_language", "")
        target_lang = data.get("target_language", "")

        lang_names = {
            "ru": "üá∑üá∫ –†—É—Å—Å–∫–∏–π",
            "en": "üá∫üá∏ –ê–Ω–≥–ª–∏–π—Å–∫–∏–π",
            "zh": "üá®üá≥ –ö–∏—Ç–∞–π—Å–∫–∏–π",
            "de": "üá©üá™ –ù–µ–º–µ—Ü–∫–∏–π"
        }

        source_name = lang_names.get(source_lang, source_lang)
        target_name = lang_names.get(target_lang, target_lang)

        metadata = data.get("translation_metadata", {})

        result = f"{header}**üåç –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω**\n"
        result += f"{source_name} ‚Üí {target_name}\n\n"

        if metadata:
            result += f"üìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**\n"
            result += f"‚Ä¢ –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {metadata.get('original_length', 0)} —Å–∏–º–≤–æ–ª–æ–≤\n"
            result += f"‚Ä¢ –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {metadata.get('translated_length', 0)} —Å–∏–º–≤–æ–ª–æ–≤\n\n"

        translated_text = data.get("translated_text", "")
        if len(translated_text) > 2000:
            preview = translated_text[:2000] + "..."
            result += f"**üìÑ –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä:**\n{preview}\n\n*(–ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ñ–∞–π–ª–µ)*"
        else:
            result += f"**üìÑ –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:**\n{translated_text}"

        return result

    def _format_ocr_result(self, header: str, data: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ OCR"""
        confidence = data.get("confidence_score", 0)
        quality = data.get("quality_analysis", {})

        confidence_emoji = "üü¢" if confidence >= 80 else "üü°" if confidence >= 60 else "üî¥"

        result = f"{header}**üëÅÔ∏è OCR —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ**\n"
        result += f"{confidence_emoji} –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1f}%\n"
        result += f"üìä –ö–∞—á–µ—Å—Ç–≤–æ: {quality.get('quality_level', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}\n\n"

        processing_info = data.get("processing_info", {})
        if processing_info:
            result += f"**üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**\n"
            result += f"‚Ä¢ –°–ª–æ–≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {processing_info.get('word_count', 0)}\n"
            result += f"‚Ä¢ –°–∏–º–≤–æ–ª–æ–≤: {processing_info.get('text_length', 0)}\n\n"

        recognized_text = data.get("recognized_text", "")
        if len(recognized_text) > 2000:
            preview = recognized_text[:2000] + "..."
            result += f"**üìÑ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:**\n{preview}\n\n*(–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ñ–∞–π–ª–µ)*"
        else:
            result += f"**üìÑ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:**\n{recognized_text}"

        recommendations = quality.get("recommendations", [])
        if recommendations:
            result += f"\n\n**üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n"
            for rec in recommendations[:2]:
                result += f"‚Ä¢ {rec}\n"

        return result

    async def cleanup_user_files(self, user_id: int, max_age_hours: int = 24):
        """–û—á–∏—Å—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        self.storage.cleanup_old_files(user_id, max_age_hours)
        self.document_chat.cleanup_old_documents(max_age_hours)