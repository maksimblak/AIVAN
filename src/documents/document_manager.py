"""
High level orchestration layer for document-related processors.
"""

from __future__ import annotations

import asyncio
import asyncio
import html
import json
import logging
import re
import tempfile
import uuid
from dataclasses import asdict
from html import escape as html_escape
from pathlib import Path
from types import MappingProxyType
from typing import Any, Awaitable, Callable, Dict, List, Mapping

from src.core.excel_export import build_risk_excel
from src.core.settings import AppSettings

from .anonymizer import DocumentAnonymizer
from .base import DocumentInfo, DocumentResult, DocumentStorage, ProcessingError
from .document_chat import DocumentChat
from .lawsuit_analyzer import LawsuitAnalyzer
from .ocr_converter import OCRConverter
from .risk_analyzer import RiskAnalyzer
from .storage_backends import ArtifactUploader, S3ArtifactUploader
from .summarizer import DocumentSummarizer
from .translator import DocumentTranslator
from .utils import TextProcessor, write_text_async

_LAWSUIT_SECTION_META: dict[str, tuple[str, str]] = {
    "demands": ("üìù", "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è"),
    "legal_basis": ("üìö", "–ü—Ä–∞–≤–æ–≤–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ"),
    "evidence": ("üìÅ", "–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞"),
    "strengths": ("‚úÖ", "–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã"),
    "risks": ("‚ö†Ô∏è", "–†–∏—Å–∫–∏ –∏ —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞"),
    "missing_elements": ("‚ùó", "–ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã"),
    "recommendations": ("üí°", "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"),
    "procedural_notes": ("üìÖ", "–ü—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏"),
}

logger = logging.getLogger(__name__)


class DocumentManager:
    """Facade that wires document processors, storage and formatting."""

    def __init__(self, openai_service=None, settings: AppSettings | None = None):
        if settings is None:
            from src.core.app_context import get_settings  # avoid circular import

            settings = get_settings()
        self._settings = settings
        self._openai_service = openai_service

        self.storage = self._init_storage(settings)
        self.summarizer = DocumentSummarizer(openai_service=openai_service, settings=settings)
        self.translator = DocumentTranslator(openai_service=openai_service, settings=settings)
        self.anonymizer = DocumentAnonymizer(settings=settings)
        self.risk_analyzer = RiskAnalyzer(openai_service=openai_service)
        self.chat = DocumentChat(openai_service=openai_service, settings=settings)
        self.ocr_converter = OCRConverter(settings=settings)
        self.lawsuit_analyzer = LawsuitAnalyzer(openai_service=openai_service)

        self._operations: Dict[str, Dict[str, Any]] = {
            "summarize": {
                "emoji": "üìë",
                "name": "–ö—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "description": "–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∫–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã",
                "formats": ["TXT", "JSON"],
                "processor": self.summarizer,
            },
            "analyze_risks": {
                "emoji": "‚ö†Ô∏è",
                "name": "–ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤",
                "description": "–í—ã—è–≤–ª–µ–Ω–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π",
                "formats": ["TXT", "JSON"],
                "processor": self.risk_analyzer,
            },
            "lawsuit_analysis": {
                "emoji": "‚öñÔ∏è",
                "name": "–ê–Ω–∞–ª–∏–∑ –∏—Å–∫–æ–≤–æ–≥–æ –∑–∞—è–≤–ª–µ–Ω–∏—è",
                "description": "–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, –ø—Ä–∞–≤–æ–≤—É—é –ø–æ–∑–∏—Ü–∏—é –∏ —Ä–∏—Å–∫–∏ –æ—Ç–∫–∞–∑–∞",
                "formats": ["MD", "JSON"],
                "processor": self.lawsuit_analyzer,
            },
            # "chat": {
            #     "emoji": "üí¨",
            #     "name": "–ß–∞—Ç —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º",
            #     "description": "–ó–∞–¥–∞–≤–∞–π—Ç–µ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É",
            #     "formats": ["interactive"],
            #     "processor": self.chat,
            # },
            "anonymize": {
                "emoji": "üï∂Ô∏è",
                "name": "–ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è",
                "description": "–£–¥–∞–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "formats": ["TXT", "JSON"],
                "processor": self.anonymizer,
            },
            # "translate": {
            #     "emoji": "üåê",
            #     "name": "–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞",
            #     "description": "–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —è–∑—ã–∫",
            #     "formats": ["TXT"],
            #     "processor": self.translator,
            # },
            "ocr": {
                "emoji": "üì∑",
                "name": "–†–∞—Å–ø–æ–∑–Ω–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞",
                "description": "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –∏ PDF",
                "formats": ["TXT"],
                "processor": self.ocr_converter,
            },
        }

        self._user_chat_sessions: Dict[int, Dict[str, Any]] = {}
        self._operations_cache_data: Dict[str, Dict[str, Any]] | None = None
        self._operations_cache_view: Mapping[str, Dict[str, Any]] | None = None

    # ------------------------------------------------------------------ API ---

    def _build_supported_operations(self) -> Dict[str, Dict[str, Any]]:
        operations: Dict[str, Dict[str, Any]] = {}
        for key, meta in self._operations.items():
            processor = meta.get("processor")
            upload_formats: List[str] = []
            if processor is not None:
                supported = getattr(processor, "supported_formats", None)
                if supported:
                    upload_formats = [fmt.lstrip(".").upper() for fmt in supported]

            descriptor: Dict[str, Any] = {
                "emoji": meta.get("emoji"),
                "name": meta.get("name"),
                "description": meta.get("description"),
                "formats": list(meta.get("formats", [])),
            }
            if upload_formats:
                descriptor["upload_formats"] = upload_formats
            operations[key] = descriptor
        return operations

    def get_supported_operations(self) -> Mapping[str, Dict[str, Any]]:
        if self._operations_cache_view is None:
            data = self._build_supported_operations()
            self._operations_cache_data = data
            self._operations_cache_view = MappingProxyType(data)
        return self._operations_cache_view

    def get_operation_info(self, operation: str) -> Dict[str, Any] | None:
        info = self.get_supported_operations().get(operation)
        if info is None:
            return None
        return dict(info)

    async def process_document(
        self,
        *,
        user_id: int,
        file_content: bytes,
        original_name: str,
        mime_type: str,
        operation: str,
        progress_callback: Callable[[dict[str, Any]], Awaitable[None]] | None = None,
        **options: Any,
    ) -> DocumentResult:
        meta = self._operations.get(operation)
        if not meta:
            return DocumentResult.error_result("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è", "UNKNOWN_OPERATION")

        try:
            doc_info = await self.storage.save_document(
                user_id=user_id,
                file_content=file_content,
                original_name=original_name,
                mime_type=mime_type,
            )
        except ProcessingError as exc:
            logger.warning("Failed to store document: %s", exc)
            return DocumentResult.error_result(exc.message, exc.error_code)
        except Exception as exc:  # noqa: BLE001
            logger.exception("Unexpected error while storing document")
            return DocumentResult.error_result(str(exc), "STORAGE_ERROR")

        if operation == "chat":
            return await self._handle_chat_upload(user_id, doc_info, options)

        processor = meta.get("processor")
        try:
            safe_kwargs = dict(options)
            if progress_callback is not None:
                safe_kwargs["progress_callback"] = progress_callback
            result = await processor.safe_process(doc_info.file_path, **safe_kwargs)
        except ProcessingError as exc:
            self._safe_unlink(doc_info.file_path)
            logger.warning("Processor error (%s): %s", operation, exc)
            return DocumentResult.error_result(exc.message, exc.error_code)
        except Exception as exc:  # noqa: BLE001
            self._safe_unlink(doc_info.file_path)
            logger.exception("Unhandled error during %s", operation)
            return DocumentResult.error_result(str(exc), "PROCESSING_ERROR")

        result.data.setdefault("document_info", asdict(doc_info))
        if result.success:
            exports = await self._prepare_exports(operation, result, doc_info)
            if exports:
                existing = list(result.data.get("exports") or [])
                result.data["exports"] = existing + exports
        else:
            result.data.setdefault("exports", [])

        self._safe_unlink(doc_info.file_path)
        return result

    def format_result_for_telegram(self, result: DocumentResult, operation: str) -> str:
        formatter = {
            "summarize": self._format_summary_result,
            "translate": self._format_translation_result,
            "anonymize": self._format_anonymize_result,
            "analyze_risks": self._format_risk_result,
            "lawsuit_analysis": self._format_lawsuit_result,
            "chat": self._format_chat_loaded,
            "ocr": self._format_ocr_result,
        }.get(operation, self._format_generic_result)
        return formatter(result.data, result.message)

    def format_chat_answer_for_telegram(self, result: DocumentResult) -> str:
        data = result.data
        answer = html_escape(data.get("answer", ""))
        confidence = float(data.get("confidence", 0.0))
        citations = data.get("citations", [])
        lines = [
            "<b>–û—Ç–≤–µ—Ç –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É</b>",
            answer or "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞",
            "",
            f"–î–æ–≤–µ—Ä–∏–µ –º–æ–¥–µ–ª–∏: {confidence:.0%}",
        ]
        if citations:
            lines.append("<b>–¶–∏—Ç–∞—Ç—ã:</b>")
            for item in citations[:5]:
                snippet = html_escape(str(item.get("snippet", "")))
                idx = item.get("chunk_index")
                lines.append(f"‚Ä¢ [chunk #{idx}] {snippet}")
        return "\n".join(lines)

    async def answer_chat_question(self, user_id: int, question: str) -> DocumentResult:
        session = self._user_chat_sessions.get(user_id)
        if not session:
            raise ProcessingError("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è —á–∞—Ç–∞", "CHAT_NOT_INITIALIZED")
        document_id = session["document_id"]
        return await self.chat.answer_question(document_id, question)

    def end_chat_session(self, user_id: int) -> bool:
        session = self._user_chat_sessions.pop(user_id, None)
        if session:
            self.chat.unload_document(session.get("document_id"))
            return True
        return False


    # -------------------------------------------------------------- helpers ---

    def _init_storage(self, settings: AppSettings) -> DocumentStorage:
        storage_root = settings.get_str("DOCUMENTS_STORAGE_PATH", None) or "data/documents"
        uploader = self._build_uploader(settings)
        return DocumentStorage(
            storage_path=storage_root,
            max_user_quota_mb=settings.document_storage_quota_mb,
            cleanup_max_age_hours=settings.document_cleanup_hours,
            cleanup_interval_seconds=settings.document_cleanup_interval_seconds,
            artifact_uploader=uploader,
        )

    def _build_uploader(self, settings: AppSettings) -> ArtifactUploader | None:
        bucket = settings.documents_s3_bucket
        if not bucket:
            return None
        return S3ArtifactUploader(
            bucket=bucket,
            prefix=settings.documents_s3_prefix,
            region_name=settings.documents_s3_region,
            endpoint_url=settings.documents_s3_endpoint,
            public_base_url=settings.documents_s3_public_url,
            acl=settings.documents_s3_acl,
        )

    async def _handle_chat_upload(
        self,
        user_id: int,
        doc_info: DocumentInfo,
        options: Dict[str, Any],
    ) -> DocumentResult:
        previous = self._user_chat_sessions.pop(user_id, None)
        if previous:
            self.chat.unload_document(previous.get("document_id"))
        try:
            document_id = await self.chat.load_document(doc_info.file_path)
        finally:
            self._safe_unlink(doc_info.file_path)
        session_payload = {
            "document_id": document_id,
            "info": asdict(doc_info),
        }
        self._user_chat_sessions[user_id] = session_payload
        data = {
            "document_id": document_id,
            "document_info": self.chat.get_document_info(document_id),
            "message": "–î–æ–∫—É–º–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —á–∞—Ç—É. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /askdoc –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ /enddoc –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.",
        }
        return DocumentResult.success_result(data=data, message="–î–æ–∫—É–º–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —á–∞—Ç—É")

    async def _prepare_exports(
        self,
        operation: str,
        result: DocumentResult,
        doc_info: DocumentInfo,
    ) -> List[Dict[str, Any]]:
        exports: List[Dict[str, Any]] = []
        base_name = Path(doc_info.original_name or "document").stem or "document"

        if operation == "summarize":
            summary = ((result.data.get("summary") or {}).get("content") or "").strip()
            structured = (result.data.get("summary") or {}).get("structured")
            if summary:
                path = await self._write_export(base_name, "summary", summary, ".txt")
                exports.append({"path": str(path), "format": "txt", "label": "Summary"})
            if structured:
                json_payload = json.dumps(structured, ensure_ascii=False, indent=2)
                path = await self._write_export(base_name, "summary", json_payload, ".json")
                exports.append({"path": str(path), "format": "json", "label": "Summary JSON"})

        elif operation == "translate":
            translated = (result.data.get("translated_text") or "").strip()
            if translated:
                path = await self._write_export(base_name, "translation", translated, ".txt")
                exports.append({"path": str(path), "format": "txt", "label": "–ü–µ—Ä–µ–≤–æ–¥"})

        elif operation == "anonymize":
            anonymized = (result.data.get("anonymized_text") or "").strip()
            report = result.data.get("anonymization_report")
            if anonymized:
                path = await self._write_export(base_name, "anonymized", anonymized, ".txt")
                exports.append({"path": str(path), "format": "txt", "label": "–ê–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"})
            if report:
                json_payload = json.dumps(report, ensure_ascii=False, indent=2)
                path = await self._write_export(base_name, "anonymized_report", json_payload, ".json")
                exports.append({"path": str(path), "format": "json", "label": "–û—Ç—á—ë—Ç"})

        elif operation == "analyze_risks":
            highlighted = (result.data.get("highlighted_text") or "").strip()
            if highlighted:
                path = await self._write_export(base_name, "risk_highlight", highlighted, ".txt")
                exports.append({"path": str(path), "format": "txt", "label": "–ü–æ–¥—Å–≤–µ—Ç–∫–∞ —Ä–∏—Å–∫–æ–≤"})
            json_payload = json.dumps(result.data, ensure_ascii=False, indent=2)
            path = await self._write_export(base_name, "risk_report", json_payload, ".json")
            exports.append({"path": str(path), "format": "json", "label": "–û—Ç—á—ë—Ç"})
            try:
                excel_path = await asyncio.to_thread(
                    build_risk_excel, result.data, file_stub=f"{base_name}_risks"
                )
                exports.append({"path": str(excel_path), "format": "xlsx", "label": "–û—Ç—á—ë—Ç (XLSX)"})
            except RuntimeError as exc:
                logger.warning("Excel export unavailable for risk analysis: %s", exc)
            except Exception as exc:  # noqa: BLE001
                logger.error("Failed to build Excel risk report: %s", exc, exc_info=True)

        elif operation == "lawsuit_analysis":
            analysis = result.data.get("analysis") or {}
            markdown = self._build_lawsuit_markdown(analysis)
            if markdown:
                path = await self._write_export(base_name, "lawsuit_analysis", markdown, ".md")
                exports.append({"path": str(path), "format": "md", "label": "–ê–Ω–∞–ª–∏–∑"})
            json_payload = json.dumps(analysis, ensure_ascii=False, indent=2)
            path = await self._write_export(base_name, "lawsuit_analysis", json_payload, ".json")
            exports.append({"path": str(path), "format": "json", "label": "–ê–Ω–∞–ª–∏–∑ (JSON)"})

        elif operation == "ocr":
            recognized = (result.data.get("recognized_text") or "").strip()
            if recognized:
                path = await self._write_export(base_name, "ocr", recognized, ".txt")
                exports.append({"path": str(path), "format": "txt", "label": "–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"})

        return exports

    async def _write_export(self, base: str, suffix: str, content: str, extension: str) -> Path:
        target = Path(tempfile.gettempdir()) / f"aivan_{base}_{suffix}_{uuid.uuid4().hex}{extension}"
        return await write_text_async(target, content)

    @staticmethod
    def _safe_unlink(path: str | Path) -> None:
        try:
            Path(path).unlink(missing_ok=True)
        except Exception:
            pass

    # ----------------------------------- formatters -----------------------------------

    def _format_summary_result(self, data: Dict[str, Any], message: str) -> str:
        summary_block = (data.get("summary") or {}).get("content") or ""
        structured = (data.get("summary") or {}).get("structured") or {}
        key_points = structured.get("key_points") or []
        deadlines = structured.get("deadlines") or []
        penalties = structured.get("penalties") or []
        checklist = structured.get("actions") or []

        lines = ["<b>–°–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞</b>"]
        if summary_block:
            lines.append(html_escape(summary_block))
        if key_points:
            lines.append("")
            lines.append("<b>–ö–ª—é—á–µ–≤—ã–µ –ø—É–Ω–∫—Ç—ã:</b>")
            for item in key_points[:8]:
                lines.append(f"‚Ä¢ {html_escape(str(item))}")
        if deadlines:
            lines.append("")
            lines.append("<b>–°—Ä–æ–∫–∏:</b>")
            for item in deadlines[:6]:
                lines.append(f"‚Ä¢ {html_escape(str(item))}")
        if penalties:
            lines.append("")
            lines.append("<b>–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å:</b>")
            for item in penalties[:6]:
                lines.append(f"‚Ä¢ {html_escape(str(item))}")
        if checklist:
            lines.append("")
            lines.append("<b>–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –¥–µ–π—Å—Ç–≤–∏—è:</b>")
            for item in checklist[:6]:
                lines.append(f"‚Ä¢ {html_escape(str(item))}")
        return "\n".join(lines)

    def _format_translation_result(self, data: Dict[str, Any], message: str) -> str:
        translated = data.get("translated_text") or ""
        src = data.get("source_language") or "?"
        tgt = data.get("target_language") or "?"
        lines = [
            "<b>–ü–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞</b>",
            f"–Ø–∑—ã–∫–æ–≤–∞—è –ø–∞—Ä–∞: {html_escape(str(src))} ‚Üí {html_escape(str(tgt))}",
            "",
            html_escape(translated)[:3500],
        ]
        return "\n".join(lines)

    def _format_anonymize_result(self, data: Dict[str, Any], message: str) -> str:
        anonymized = (data.get("anonymized_text") or "")[:3500]
        report = data.get("anonymization_report") or {}
        total = report.get("total_matches", 0)
        stats = report.get("counters") or {}
        lines = [
            "<b>–ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è</b>",
            f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {int(total)}",
            "",
            html_escape(anonymized),
        ]
        if stats:
            lines.append("")
            lines.append("<b>–ö–∞—Ç–µ–≥–æ—Ä–∏–∏:</b>")
            for key, value in list(stats.items())[:8]:
                lines.append(f"‚Ä¢ {html_escape(str(key))}: {value}")
        return "\n".join(lines)

    def _format_risk_result(self, data: Dict[str, Any], message: str) -> str:
        overall = data.get("overall_risk_level") or "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"
        recommendations = data.get("recommendations") or []
        pattern_risks = data.get("pattern_risks") or []
        lines = [
            "<b>–ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤</b>",
            f"–û–±—â–∏–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞: {html_escape(str(overall))}",
        ]
        if pattern_risks:
            lines.append("")
            lines.append("<b>–í–∞–∂–Ω—ã–µ –Ω–∞—Ö–æ–¥–∫–∏:</b>")
            for item in pattern_risks[:5]:
                desc = item.get("description") or ""
                level = item.get("risk_level") or ""
                lines.append(f"‚Ä¢ {html_escape(str(level))}: {html_escape(desc)}")
        if recommendations:
            lines.append("")
            lines.append("<b>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:</b>")
            for rec in recommendations[:6]:
                lines.append(f"‚Ä¢ {html_escape(str(rec))}")
        return "\n".join(lines)

    def _format_lawsuit_result(self, data: Dict[str, Any], message: str) -> str:
        analysis = data.get("analysis") or {}
        lines: List[str] = ["<b>–ê–Ω–∞–ª–∏–∑ –∏—Å–∫–æ–≤–æ–≥–æ –∑–∞—è–≤–ª–µ–Ω–∏—è</b>"]

        summary = str(analysis.get("summary") or "").strip()
        if summary:
            lines.append(html_escape(summary))

        parties = analysis.get("parties") or {}
        party_lines: List[str] = []
        plaintiff = str(parties.get("plaintiff") or "").strip()
        defendant = str(parties.get("defendant") or "").strip()
        if plaintiff:
            party_lines.append(f"‚Ä¢ –ò—Å—Ç–µ—Ü: {html_escape(plaintiff)}")
        if defendant:
            party_lines.append(f"‚Ä¢ –û—Ç–≤–µ—Ç—á–∏–∫: {html_escape(defendant)}")
        for item in parties.get("other") or []:
            text = str(item or "").strip()
            if text:
                party_lines.append(f"‚Ä¢ –£—á–∞—Å—Ç–Ω–∏–∫: {html_escape(text)}")
        if party_lines:
            lines.append("")
            lines.append("<b>üë• –°—Ç–æ—Ä–æ–Ω—ã:</b>")
            lines.append("\n".join(party_lines))

        def _section(key: str, values: Any) -> None:
            cleaned = [str(value or "").strip() for value in (values or []) if str(value or "").strip()]
            if not cleaned:
                return
            icon, title = _LAWSUIT_SECTION_META[key]
            lines.append("")
            lines.append(f"<b>{icon} {title}:</b>")
            lines.append("\n".join(f"‚Ä¢ {html_escape(item)}" for item in cleaned))

        for section_key in (
            "demands",
            "legal_basis",
            "evidence",
            "strengths",
            "risks",
            "missing_elements",
            "recommendations",
            "procedural_notes",
        ):
            _section(section_key, analysis.get(section_key))

        confidence = str(analysis.get("confidence") or "").strip()
        if confidence:
            lines.append("")
            lines.append(f"<i>–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞: {html_escape(confidence)}</i>")

        if data.get("truncated"):
            lines.append("")
            lines.append("<i>‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω –ø–æ —É—Å–µ—á—ë–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞.</i>")

        if (result_exports := data.get("exports")):
            if result_exports:
                lines.append("")
                lines.append("<i>üìé –ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ñ–∞–π–ª—ã –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω—ã –Ω–∏–∂–µ.</i>")

        if message:
            lines.append("")
            lines.append(html_escape(message))

        html_body = "\n\n".join(part for part in lines if part)
        plain_body = re.sub(r"<br>\s*", "\n", html_body)
        plain_body = re.sub(r"</?b>", "", plain_body)
        plain_body = re.sub(r"</?i>", "", plain_body)
        plain_body = re.sub(r"</?u>", "", plain_body)
        plain_body = re.sub(r"</?code>", "", plain_body)
        plain_body = html.unescape(plain_body)
        return plain_body

    def _format_chat_loaded(self, data: Dict[str, Any], message: str) -> str:
        info = data.get("document_info") or {}
        metadata = info.get("metadata") or {}
        doc_id = data.get("document_id")
        top_keywords: List[str] = []
        if doc_id and doc_id in self.chat.loaded_documents:
            try:
                source_text = self.chat.loaded_documents[doc_id]["text"]
                top_keywords = TextProcessor.top_keywords(source_text, limit=5)
            except Exception:
                top_keywords = []
        lines = [
            "<b>–î–æ–∫—É–º–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —á–∞—Ç—É</b>",
            "–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º –≤ —ç—Ç–æ–º –¥–∏–∞–ª–æ–≥–µ. –î–ª—è –≤—ã—Ö–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ /cancel",
            "",
            f"–û–±—ä–µ–º: {metadata.get('words', 0)} —Å–ª–æ–≤",
            f"–ß–∏—Å–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {metadata.get('sentences', 0)}",
        ]
        if top_keywords:
            lines.append("")
            lines.append("<b>–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:</b> " + ", ".join(top_keywords))
        return "\n".join(lines)

    def _format_ocr_result(self, data: Dict[str, Any], message: str) -> str:
        text = (data.get("recognized_text") or "")[:3500]
        confidence = float(data.get("confidence_score", 0.0))
        lines = [
            "<b>–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç</b>",
            f"–¢–æ—á–Ω–æ—Å—Ç—å: {confidence:.1f}%",
            "",
            html_escape(text),
        ]
        return "\n".join(lines)

    def _format_generic_result(self, data: Dict[str, Any], message: str) -> str:
        return html_escape(message or "–ì–æ—Ç–æ–≤–æ")

    @staticmethod
    def _build_lawsuit_markdown(analysis: Dict[str, Any]) -> str:
        lines = ["# –ê–Ω–∞–ª–∏–∑ –∏—Å–∫–æ–≤–æ–≥–æ –∑–∞—è–≤–ª–µ–Ω–∏—è", ""]

        summary = str(analysis.get("summary") or "").strip()
        if summary:
            lines.extend(["## ‚öñÔ∏è –†–µ–∑—é–º–µ", summary, ""])

        parties = analysis.get("parties") or {}
        party_lines: list[str] = []
        if parties.get("plaintiff"):
            party_lines.append(f"- –ò—Å—Ç–µ—Ü: {parties['plaintiff']}")
        if parties.get("defendant"):
            party_lines.append(f"- –û—Ç–≤–µ—Ç—á–∏–∫: {parties['defendant']}")
        for item in parties.get("other") or []:
            text = str(item or "").strip()
            if text:
                party_lines.append(f"- –£—á–∞—Å—Ç–Ω–∏–∫: {text}")
        if party_lines:
            lines.extend(["## üë• –°—Ç–æ—Ä–æ–Ω—ã", *party_lines, ""])

        def append_block(key: str, values: Any) -> None:
            cleaned = [str(value or "").strip() for value in (values or []) if str(value or "").strip()]
            if not cleaned:
                return
            icon, title = _LAWSUIT_SECTION_META[key]
            lines.append(f"## {icon} {title}")
            for entry in cleaned:
                lines.append(f"- {entry}")
            lines.append("")

        for section_key in (
            "demands",
            "legal_basis",
            "evidence",
            "strengths",
            "risks",
            "missing_elements",
            "recommendations",
            "procedural_notes",
        ):
            append_block(section_key, analysis.get(section_key))

        confidence = str(analysis.get("confidence") or "").strip()
        if confidence:
            lines.extend(["", f"_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞: {confidence}_"])

        return "\n".join(lines).strip()
