# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ß–∞–Ω–∫–∏–Ω–≥ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º + —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø–∞–¥–µ–Ω–∏—è–º —á–∞—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤.
- –†–µ–∂–∏–º—ã: –æ–Ω–ª–∞–π–Ω (LLM —á–µ—Ä–µ–∑ openai_service) –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –æ—Ñ—Ñ–ª–∞–π–Ω-–¥–µ–≥—Ä–∞–¥ (–≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω–∏–∫).
- –ì–ª–æ—Å—Å–∞—Ä–∏–π (–∫–∞—Å—Ç–æ–º–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã), –∑–∞—â–∏—Ç–∞ –Ω–µ-–ø–µ—Ä–µ–≤–æ–¥–∏–º—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (URL/Email/–¥–∞—Ç—ã/—á–∏—Å–ª–∞).
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (—Å–ø–∏—Å–∫–∏, –∑–∞–≥–æ–ª–æ–≤–∫–∏), —É–≤–∞–∂–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ —á–∞–Ω–∫–∞–º: –ø—Ä–µ–≤—å—é, –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Å—Ç–∞—Ç—É—Å.
- –ê–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç —è–∑—ã–∫–∞ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–æ–∫–∏—Ä–æ–≤–∫–∞ —è–∑—ã–∫–æ–≤ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
"""

from __future__ import annotations

import logging
from src.core.settings import AppSettings

import re
import time
from pathlib import Path
from typing import Any, Dict, List, Tuple

from .base import DocumentProcessor, DocumentResult, ProcessingError
from .utils import FileFormatHandler, TextProcessor

logger = logging.getLogger(__name__)

# ----------------------------- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —è–∑—ã–∫–æ–≤ -----------------------------

SUPPORTED_LANGUAGES: Dict[str, str] = {
    "ru": "–†—É—Å—Å–∫–∏–π",
    "en": "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π",
    "de": "–ù–µ–º–µ—Ü–∫–∏–π",
    "fr": "–§—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π",
    "es": "–ò—Å–ø–∞–Ω—Å–∫–∏–π",
    "it": "–ò—Ç–∞–ª—å—è–Ω—Å–∫–∏–π",
    "pt": "–ü–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π",
    "zh": "–ö–∏—Ç–∞–π—Å–∫–∏–π",
    "ja": "–Ø–ø–æ–Ω—Å–∫–∏–π",
}

LANG_NAMES: Dict[str, str] = {
    "ru": "üá∑üá∫ –†—É—Å—Å–∫–∏–π",
    "en": "üá¨üáß –ê–Ω–≥–ª–∏–π—Å–∫–∏–π",
    "de": "üá©üá™ –ù–µ–º–µ—Ü–∫–∏–π",
    "fr": "üá´üá∑ –§—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π",
    "es": "üá™üá∏ –ò—Å–ø–∞–Ω—Å–∫–∏–π",
    "it": "üáÆüáπ –ò—Ç–∞–ª—å—è–Ω—Å–∫–∏–π",
    "pt": "üáµüáπ –ü–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π",
    "zh": "üá®üá≥ –ö–∏—Ç–∞–π—Å–∫–∏–π",
    "ja": "üáØüáµ –Ø–ø–æ–Ω—Å–∫–∏–π",
}

def _human_lang(code: str) -> str:
    return LANG_NAMES.get(code, code)


# ------------------------------- –ü—Ä–æ–º–ø—Ç LLM -------------------------------

TRANSLATION_SYSTEM_PROMPT = """
–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –°–æ—Ö—Ä–∞–Ω—è–π —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –∏ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å.
- –°–æ—Ö—Ä–∞–Ω—è–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞: –∑–∞–≥–æ–ª–æ–≤–∫–∏, –Ω—É–º–µ—Ä–∞—Ü–∏—é, —Å–ø–∏—Å–∫–∏, –ø–æ–¥–ø—É–Ω–∫—Ç—ã.
- –°–æ—Ö—Ä–∞–Ω—è–π —Ä–∞–∑–º–µ—Ç–∫—É –∏ —Å–ø–µ—Ü. –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –≤–∏–¥–∞ {{T0}}, {{T1}}, {{URL0}} ‚Äî –ù–ï –ü–ï–†–ï–í–û–î–ò –∏—Ö –∏ –Ω–µ —É–¥–∞–ª—è–π.
- –ù–µ —Å–æ–∫—Ä–∞—â–∞–π –∏ –Ω–µ –¥–æ–±–∞–≤–ª—è–π –ª–∏—à–Ω–∏—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π, –µ—Å–ª–∏ –Ω–µ –ø–æ–ø—Ä–æ—Å–∏–ª–∏.
- –ï—Å–ª–∏ –≤–∏–¥–∏—à—å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Ç–µ—Ä–º–∏–Ω –∏–∑ –≥–ª–æ—Å—Å–∞—Ä–∏—è, —Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—É—é —Ñ–æ—Ä–º—É –ø–µ—Ä–µ–≤–æ–¥–∞.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
‚Äî –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –ø–µ—Ä–µ–≤–æ–¥ (plain text/markdown), –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
"""

TRANSLATION_USER_TMPL = """
–ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫: {src_name}
–¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫: {tgt_name}

–ì–ª–æ—Å—Å–∞—Ä–∏–π (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω):
{glossary_text}

–ü–µ—Ä–µ–≤–µ–¥–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç, —Å–æ—Ö—Ä–∞–Ω–∏–≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã:
---
{payload}
---
"""


# ----------------------------- –ó–∞—â–∏—Ç–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π -----------------------------

URL_RE = re.compile(r"(https?://[^\s]+)")
EMAIL_RE = re.compile(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,24}\b")
DATE_RE = re.compile(
    r"\b(\d{1,2}[./-]\d{1,2}[./-]\d{2,4}|\d{4}-\d{2}-\d{2}|\d{1,2}\s+(?:—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è|January|February|March|April|May|June|July|August|September|October|November|December))\b",
    re.IGNORECASE,
)
NUMBER_RE = re.compile(r"\b\d[\d\s.,]*\b")

def _protect_entities(text: str) -> Tuple[str, Dict[str, str]]:
    """–ó–∞–º–µ–Ω—è–µ–º –∫—Ä–∏—Ç–∏—á–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞–º–∏ {{URL0}}, {{EM0}}, {{DT0}}, {{N0}}."""
    mapping: Dict[str, str] = {}
    counter = {"URL": 0, "EM": 0, "DT": 0, "N": 0}

    # —ç—Ç–∞–ø–Ω–æ –∑–∞–º–µ–Ω—è–µ–º (–≤–∞–∂–µ–Ω –ø–æ—Ä—è–¥–æ–∫: URL/Email –¥–æ —á–∏—Å–µ–ª/–¥–∞—Ç)
    text_map = {"URL": text, "EM": "", "DT": "", "N": ""}
    text_map["EM"] = URL_RE.sub(lambda m: m.group(0), text_map["URL"])  # no-op, –¥–µ—Ä–∂–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    text_map["DT"] = text_map["EM"]
    text_map["N"] = text_map["DT"]

    # 1) URL
    s1 = URL_RE.sub(lambda m: _swap_store(mapping, "URL", counter, m.group(0)), text_map["URL"])
    # 2) EMAIL
    s2 = EMAIL_RE.sub(lambda m: _swap_store(mapping, "EM", counter, m.group(0)), s1)
    # 3) DATE
    s3 = DATE_RE.sub(lambda m: _swap_store(mapping, "DT", counter, m.group(0)), s2)
    # 4) NUMBER
    s4 = NUMBER_RE.sub(lambda m: _swap_store(mapping, "N", counter, m.group(0)), s3)
    return s4, mapping

def _swap_store(mapping: Dict[str, str], tag: str, counter: Dict[str, int], value: str) -> str:
    key = f"{{{{{tag}{counter[tag]}}}}}"
    mapping[key] = value
    counter[tag] += 1
    return key

def _restore_entities(text: str, mapping: Dict[str, str]) -> str:
    # —á—Ç–æ–±—ã –Ω–µ –ø–æ–≤—Ä–µ–¥–∏—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è, –∑–∞–º–µ–Ω—è–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –¥–ª–∏–Ω—ã –∫–ª—é—á–∞
    for k in sorted(mapping.keys(), key=len, reverse=True):
        text = text.replace(k, mapping[k])
    return text


# ------------------------------- –ö–ª–∞—Å—Å –º–æ–¥—É–ª—è -------------------------------

class DocumentTranslator(DocumentProcessor):
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""

    def __init__(self, openai_service=None, settings: AppSettings | None = None):
        super().__init__(name="DocumentTranslator", max_file_size=50 * 1024 * 1024)
        self.supported_formats = [".pdf", ".docx", ".doc", ".txt"]
        self.openai_service = openai_service
        self.supported_languages = SUPPORTED_LANGUAGES.copy()

        if settings is None:
            from src.core.app_context import get_settings  # avoid circular import

            settings = get_settings()
        self._settings = settings

        self.allow_ai = settings.get_bool("TRANSLATE_ALLOW_AI", True)
        self.chunk_size = settings.get_int("TRANSLATE_CHUNK_SIZE", 4000)
        self.chunk_overlap = settings.get_int("TRANSLATE_CHUNK_OVERLAP", 300)
        self.max_retries = settings.get_int("TRANSLATE_MAX_RETRIES", 2)

    async def process(
        self,
        file_path: str | Path,
        source_lang: str = "ru",
        target_lang: str = "en",
        *,
        glossary: Dict[str, str] | None = None,
        **kwargs: Any,
    ) -> DocumentResult:
        """
        –ü–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Args:
            file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            source_lang: –∏—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫ (ru, en, zh, de, ...)
            target_lang: —Ü–µ–ª–µ–≤–æ–π —è–∑—ã–∫ (ru, en, zh, de, ...)
            glossary: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –≥–ª–æ—Å—Å–∞—Ä–∏–π {–∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ—Ä–º–∏–Ω: —Ü–µ–ª–µ–≤–æ–π —Ç–µ—Ä–º–∏–Ω}
        """
        if not self.allow_ai or not self.openai_service:
            if not self.openai_service:
                logger.warning("OpenAI service is not initialized; translation will fallback to identity")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        success, text = await FileFormatHandler.extract_text_from_file(file_path)
        if not success:
            raise ProcessingError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç: {text}", "EXTRACTION_ERROR")

        cleaned_text = TextProcessor.clean_text(text)
        if not cleaned_text.strip():
            raise ProcessingError("–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞", "EMPTY_DOCUMENT")

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤
        src = (source_lang or "").lower()
        tgt = (target_lang or "").lower()
        if src in {"auto", "detect", ""}:
            src = self.detect_language(cleaned_text)

        if src not in self.supported_languages:
            src = self.detect_language(cleaned_text)
        if tgt not in self.supported_languages:
            # –¥–µ—Ñ–æ–ª—Ç ‚Äî –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–π –æ—Ç –¥–µ—Ç–µ–∫—Ç–∞
            tgt = "en" if src == "ru" else "ru"

        # –ï—Å–ª–∏ —è–∑—ã–∫–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ ‚Äî –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º
        if src == tgt:
            return DocumentResult.success_result(
                data={
                    "translated_text": cleaned_text,
                    "source_language": src,
                    "target_language": tgt,
                    "original_file": str(file_path),
                    "chunk_details": [],
                    "translation_metadata": {
                        "original_length": len(cleaned_text),
                        "translated_length": len(cleaned_text),
                        "language_pair": f"{src} -> {tgt}",
                        "chunks_processed": 0,
                        "mode": "identity",
                    },
                },
                message=f"–ò—Å—Ö–æ–¥–Ω—ã–π –∏ —Ü–µ–ª–µ–≤–æ–π —è–∑—ã–∫–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç ({_human_lang(src)}). –ü–µ—Ä–µ–≤–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.",
            )

        # –ü–µ—Ä–µ–≤–æ–¥
        translated_text, chunk_details = await self._translate_text(
            cleaned_text, src, tgt, glossary=glossary or {}
        )

        result_data = {
            "translated_text": translated_text,
            "source_language": src,
            "target_language": tgt,
            "original_file": str(file_path),
            "chunk_details": chunk_details,
            "translation_metadata": {
                "original_length": len(cleaned_text),
                "translated_length": len(translated_text),
                "language_pair": f"{src} -> {tgt}",
                "chunks_processed": len(chunk_details),
                "mode": "ai" if self.allow_ai and self.openai_service else "fallback",
            },
        }

        return DocumentResult.success_result(
            data=result_data,
            message=f"–ü–µ—Ä–µ–≤–æ–¥ —Å {self.supported_languages[src]} –Ω–∞ {self.supported_languages[tgt]} –∑–∞–≤–µ—Ä—à—ë–Ω",
        )

    # -------------------------------- –ü–µ—Ä–µ–≤–æ–¥ --------------------------------

    async def _translate_text(
        self, text: str, source_lang: str, target_lang: str, *, glossary: Dict[str, str]
    ) -> Tuple[str, List[Dict[str, Any]]]:
        """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é AI (—á–∞–Ω–∫–∏–Ω–≥ + –∑–∞—â–∏—Ç–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π)."""
        src_name = self.supported_languages.get(source_lang, source_lang)
        tgt_name = self.supported_languages.get(target_lang, target_lang)

        # –ó–∞—â–∏—Ç–∏–º —Å—É—â–Ω–æ—Å—Ç–∏ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞–º–∏
        protected_text, placeholders = _protect_entities(text)

        # –†–∞–∑—Ä–µ–∂–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = TextProcessor.split_into_chunks(protected_text, max_chunk_size=self.chunk_size, overlap=self.chunk_overlap)
        if not chunks:
            chunks = [protected_text]

        details: List[Dict[str, Any]] = []
        translated_parts: List[str] = []

        # –ï—Å–ª–∏ –Ω–µ—Ç AI ‚Äî –¥–µ–≥—Ä–∞–¥–∏—Ä—É–µ–º: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (—Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º —Å—É—â–Ω–æ—Å—Ç–µ–π)
        if not (self.allow_ai and self.openai_service):
            return _restore_entities(protected_text, placeholders), []

        glossary_text = self._format_glossary(glossary)

        for i, chunk in enumerate(chunks, 1):
            t0 = time.perf_counter()
            attempt = 0
            translated_part = None
            error_msg = None

            while attempt <= self.max_retries:
                attempt += 1
                try:
                    user_payload = TRANSLATION_USER_TMPL.format(
                        src_name=src_name,
                        tgt_name=tgt_name,
                        glossary_text=glossary_text or "(–Ω–µ —É–∫–∞–∑–∞–Ω)",
                        payload=chunk,
                    )
                    resp = await self.openai_service.ask_legal(
                        system_prompt=TRANSLATION_SYSTEM_PROMPT,
                        user_text=user_payload,
                    )
                    if resp and resp.get("ok"):
                        translated_part = (resp.get("text") or "").strip()
                        if translated_part:
                            break
                        else:
                            error_msg = "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏"
                    else:
                        error_msg = "–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –æ—à–∏–±–∫—É"
                except Exception as e:
                    error_msg = f"{type(e).__name__}: {e}"
                # —Ä–µ—Ç—Ä–∞–π ‚Äî –ø—Ä–æ—Å—Ç–æ –∏–¥—ë–º –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –ø–æ–ø—ã—Ç–∫—É

            dt = time.perf_counter() - t0

            if translated_part is None:
                # –§–æ–ª–±—ç–∫: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –±–ª–æ–∫ (–Ω–æ –ø–æ—Ç–æ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–º —Å—É—â–Ω–æ—Å—Ç–∏)
                translated_part = chunk
                status = "fallback"
            else:
                status = "ok"

            translated_parts.append(translated_part)
            details.append(
                {
                    "chunk_number": i,
                    "status": status,
                    "duration_sec": round(dt, 3),
                    "source_preview": chunk[:200],
                    "translated_preview": translated_part[:200],
                    "attempts": attempt,
                    "error": error_msg,
                }
            )

        # –°–∫–ª–µ–∏–≤–∞–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –Ω–∞–∑–∞–¥
        merged = self._merge_chunks(translated_parts)
        restored = _restore_entities(merged, placeholders)
        return restored, details

    @staticmethod
    def _format_glossary(glossary: Dict[str, str]) -> str:
        if not glossary:
            return ""
        # –ü—Ä–æ—Å—Ç–∞—è —Ç–∞–±–ª–∏—á–∫–∞ "–∏—Å—Ö–æ–¥–Ω—ã–π => —Ü–µ–ª–µ–≤–æ–π"
        pairs = [f"- {src} ‚áí {dst}" for src, dst in glossary.items()]
        return "\n".join(pairs)

    @staticmethod
    def _merge_chunks(chunks: List[str]) -> str:
        """–ú—è–≥–∫–∞—è —Å–∫–ª–µ–π–∫–∞: –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏, —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—ã."""
        cleaned = [c.strip() for c in chunks if c is not None]
        return "\n\n".join(cleaned).strip()

    # ------------------------------ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ ------------------------------
