# telegram_legal_bot/handlers/legal_query.py
from __future__ import annotations

import logging
from collections import defaultdict, deque
from typing import Any, Deque, Dict, List, Optional

from aiogram import F, Router, types
from aiogram.exceptions import TelegramBadRequest
from aiogram.utils.chat_action import ChatActionSender  # aiogram v3

from telegram_legal_bot.config import Settings
from telegram_legal_bot.services import OpenAIService
from telegram_legal_bot.utils.rate_limiter import RateLimiter
from telegram_legal_bot.utils.message_formatter import (
    build_legal_reply,
    chunk_markdown_v2,
    strip_md2_escapes,
)

router = Router(name="legal_query")
log = logging.getLogger("legal_query")

# –ö–æ–Ω—Ç–µ–∫—Å—Ç, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –∏–∑ main.py
_settings: Optional[Settings] = None
_ai: Optional[OpenAIService] = None
_rl: Optional[RateLimiter] = None
_history: Dict[int, Deque[Dict[str, str]]] = defaultdict(lambda: deque(maxlen=10))


def setup_context(settings: Settings, ai: OpenAIService) -> None:
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Ö—ç–Ω–¥–ª–µ—Ä–∞ (–≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ main.py).
    """
    global _settings, _ai, _rl, _history
    _settings = settings
    _ai = ai
    _rl = RateLimiter(max_calls=settings.max_requests_per_hour, period_seconds=3600)

    # –•—Ä–∞–Ω–∏–º —Ä–æ–≤–Ω–æ N –æ–±–º–µ–Ω–æ–≤ (user‚Üîassistant), –∑–Ω–∞—á–∏—Ç maxlen = N * 2
    pairs = max(1, int(settings.history_size))
    _history = defaultdict(lambda: deque(maxlen=pairs * 2))


def _fmt_md(text: str) -> str:
    """
    –ú–∏–Ω–∏-—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥ MarkdownV2, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –Ω–µ —Ä–æ–Ω—è—Ç—å —Ñ–æ—Ä–º–∞—Ç.
    """
    if not text:
        return ""
    # –ø–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω: —Å–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª–µ—à
    text = text.replace("\\", "\\\\")
    for ch in ("_", "*", "[", "]", "(", ")", "~", "`", ">", "#", "+", "-", "=", "|", "{", "}", ".", "!"):
        text = text.replace(ch, "\\" + ch)
    return text


def _schema_to_markdown(d: Dict[str, Any]) -> str:
    """
    –†–µ–Ω–¥–µ—Ä–∏—Ç –æ—Ç–≤–µ—Ç –ø–æ LEGAL_SCHEMA_V2 –≤ —á–∏—Ç–∞–µ–º—ã–π MarkdownV2.
    –ü–æ–ª—è —Å—Ö–µ–º—ã –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã ‚Äî –≤—ã–≤–æ–¥–∏–º —Ç–æ, —á—Ç–æ –ø—Ä–∏—à–ª–æ.
    """
    lines: List[str] = []

    conclusion = (d.get("conclusion") or "").strip()
    if conclusion:
        lines.append(f"*–ö—Ä–∞—Ç–∫–æ:* {_fmt_md(conclusion)}")

    # –ü–æ–¥–±–æ—Ä–∫–∞ –¥–µ–ª
    cases = d.get("cases") or []
    if isinstance(cases, list) and cases:
        lines.append("\n*–ü–æ–¥–±–æ—Ä–∫–∞ –¥–µ–ª:*")
        for c in cases:
            court = _fmt_md(str(c.get("court", "")))
            date = _fmt_md(str(c.get("date", "")))
            case_no = _fmt_md(str(c.get("case_no", "")))
            url = str(c.get("url", "") or "")
            holding = _fmt_md(str(c.get("holding", "")))
            facts = _fmt_md(str(c.get("facts", "")))

            head = f"‚Ä¢ {court}, {date}, ‚Ññ {case_no}"
            if url:
                # —Å—Å—ã–ª–∫–∞ –≤ –∫—Ä—É–≥–ª—ã—Ö —Å–∫–æ–±–∫–∞—Ö ‚Äî —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º –∑–∞—Ä–∞–Ω–µ–µ
                head += f" ‚Äî [—Å—Å—ã–ª–∫–∞]({url})"
            lines.append(head)
            if holding:
                lines.append(f"  ‚îî –ò—Å—Ö–æ–¥: {holding}")
            if facts:
                lines.append(f"  ‚îî –§–∞–±—É–ª–∞: {facts}")

    # –ù–æ—Ä–º—ã –ø—Ä–∞–≤–∞
    legal_basis = d.get("legal_basis") or []
    if isinstance(legal_basis, list) and legal_basis:
        lines.append("\n*–ù–æ—Ä–º—ã –ø—Ä–∞–≤–∞:*")
        for n in legal_basis:
            act = _fmt_md(str(n.get("act", "")))
            article = _fmt_md(str(n.get("article", "")))
            lines.append(f"‚Ä¢ {act}, {article}")

    # –ê–Ω–∞–ª–∏—Ç–∏–∫–∞
    analysis = (d.get("analysis") or "").strip()
    if analysis:
        lines.append("\n*–ê–Ω–∞–ª–∏—Ç–∏–∫–∞:*")
        lines.append(_fmt_md(analysis))

    # –†–∏—Å–∫–∏
    risks = d.get("risks") or []
    if isinstance(risks, list) and risks:
        lines.append("\n*–†–∏—Å–∫–∏:*")
        for r in risks:
            lines.append(f"‚Ä¢ {_fmt_md(str(r))}")

    # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
    steps = d.get("next_actions") or []
    if isinstance(steps, list) and steps:
        lines.append("\n*–®–∞–≥–∏:*")
        for s in steps:
            lines.append(f"‚Ä¢ {_fmt_md(str(s))}")

    # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ (–º–∏–Ω–∏–º—É–º 2 –¥–æ–º–µ–Ω–∞ –æ–∂–∏–¥–∞–µ—Ç—Å—è —Å—Ö–µ–º–æ–π, –Ω–æ —Ä–µ–Ω–¥–µ—Ä–∏–º –≤—Å—ë, —á—Ç–æ –ø—Ä–∏—à–ª–æ)
    sources = d.get("sources") or []
    if isinstance(sources, list) and sources:
        lines.append("\n*–ò—Å—Ç–æ—á–Ω–∏–∫–∏:*")
        for s in sources:
            title = _fmt_md(str(s.get("title", "") or "–ò—Å—Ç–æ—á–Ω–∏–∫"))
            url = str(s.get("url", "") or "")
            if url:
                lines.append(f"‚Ä¢ [{title}]({url})")
            else:
                lines.append(f"‚Ä¢ {title}")

    # –£—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∏—Ö –≤–µ—Ä–Ω—É–ª–∞)
    clar = d.get("clarifications") or []
    if isinstance(clar, list) and clar:
        lines.append("\n*–ß—Ç–æ –µ—â—ë –Ω—É–∂–Ω–æ —É—Ç–æ—á–Ω–∏—Ç—å:*")
        for q in clar:
            lines.append(f"‚Ä¢ {_fmt_md(str(q))}")

    return "\n".join(lines).strip() or _fmt_md(d.get("conclusion") or "")


@router.message(F.text & ~F.text.startswith("/"))
async def handle_legal_query(message: types.Message) -> None:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π —Ö—ç–Ω–¥–ª–µ—Ä —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.
    ‚Äî –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª–∏–Ω—ã
    ‚Äî Rate-limit –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    ‚Äî –ò–Ω–¥–∏–∫–∞—Ü–∏—è ¬´–ø–µ—á–∞—Ç–∞–µ—Ç‚Ä¶¬ª
    ‚Äî –û—Ç–≤–µ—Ç + —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ 4096
    """
    assert _settings is not None and _ai is not None and _rl is not None

    user_id = message.from_user.id if message.from_user else 0
    chat_id = message.chat.id
    text = (message.text or "").strip()

    log.info("IN: user=%s chat=%s len=%s", user_id, chat_id, len(text))

    # –ú–∏–Ω–∏-–¥–ª–∏–Ω–∞
    if len(text) < _settings.min_question_length:
        await message.answer(
            "‚úã –í–æ–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–ø–∏—à–∏—Ç–µ —Å–∏—Ç—É–∞—Ü–∏—é –ø–æ–¥—Ä–æ–±–Ω–µ–π.",
            parse_mode=None,
        )
        return

    # Rate-limit
    if not await _rl.check(user_id):
        remain = await _rl.remaining(user_id)
        msg = "‚è≥ –õ–∏–º–∏—Ç –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –±–ª–∏–∂–∞–π—à–∏–π —á–∞—Å –∏—Å—á–µ—Ä–ø–∞–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        if remain:
            msg += f" –î–æ—Å—Ç—É–ø–Ω–æ –µ—â—ë: {remain}."
        await message.answer(msg, parse_mode=None)
        return

    short_history: List[Dict[str, str]] = list(_history[user_id])

    try:
        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä ¬´–ø–µ—á–∞—Ç–∞–µ—Ç‚Ä¶¬ª
        async with ChatActionSender.typing(bot=message.bot, chat_id=chat_id):
            # 1) –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–æ–≥–∏–π –æ—Ç–≤–µ—Ç –ø–æ LEGAL_SCHEMA_V2
            md_text: Optional[str] = None
            assistant_summary: str = ""

            try:
                rich = await _ai.ask_ivan(text)
                data = rich.get("data")
                if isinstance(data, dict) and (data.get("conclusion") or data.get("sources") or data.get("cases")):
                    md_text = _schema_to_markdown(data)
                    assistant_summary = (data.get("conclusion") or "")[:1000]
            except Exception as e_json:
                log.warning("ask_ivan failed, fallback to simple path: %r", e_json)

            # 2) –§–æ–ª–±—ç–∫: —É–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø—É—Ç—å (answer + laws)
            if not md_text:
                result = await _ai.generate_legal_answer(text, short_history=short_history)
                answer: str = result.get("answer") or ""
                laws: List[str] = result.get("laws") or []
                assistant_summary = answer[:1000]
                md_text = build_legal_reply(answer=answer, laws=laws)

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ—Ä–æ—Ç–∫—É—é –∏—Å—Ç–æ—Ä–∏—é (user‚Üíassistant)
        _history[user_id].append({"role": "user", "content": text})
        _history[user_id].append({"role": "assistant", "content": assistant_summary})

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ –∫—É—Å–∫–∞–º
        chunks = chunk_markdown_v2(md_text, limit=4096)
        for part in chunks:
            try:
                await message.answer(part, parse_mode=_settings.parse_mode)
            except TelegramBadRequest:
                # —Ñ–æ–ª–±—ç–∫ –µ—Å–ª–∏ MarkdownV2 —Å–ª–æ–º–∞–ª—Å—è ‚Äî —É–±–∏—Ä–∞–µ–º —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
                await message.answer(strip_md2_escapes(part), parse_mode=None)

        log.info("OUT: user=%s chat=%s sent_chunks=%s", user_id, chat_id, len(chunks))

    except TelegramBadRequest as e:
        log.exception("TelegramBadRequest: user=%s chat=%s err=%r", user_id, chat_id, e)
        await message.answer(
            "üòï –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏—è. –û—Ç–ø—Ä–∞–≤–ª—è—é –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏.",
            parse_mode=None,
        )
    except Exception as e:
        log.exception("LLM handler error: user=%s chat=%s err=%r", user_id, chat_id, e)
        await message.answer(
            "üòï –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ —á–µ—Ä–µ–∑ –ø–∞—Ä—É –º–∏–Ω—É—Ç.",
            parse_mode=None,
        )
